---
title: "STA243 Hw3"
author:
- Eric Kalosa-Kenyon
- Justin Wang
date: "`r format(Sys.time(), '%d %B %Y')`"
abstract: This assignment enforces the concepts and methods of expectation
    maximization and efficient sampling.
output:
  html_document:
    toc: yes
---


# 1
## 1.a
We first differentiate the log-likelihood and obtain
$$
\frac{d\;l(\theta)}{d\;l\theta} = \frac{x_1}{2+\theta} -
\frac{x_2+x_3}{1-\theta} + x_4 = 0
$$
$$
\iff \frac{x_1}{2+\theta} = \frac{(x_2 + x_3)\theta -
(1-\theta)x_4}{(1-\theta)\theta}
$$
$$
\iff \theta x_1 - \theta^2x_1 = (x_2+x_3)(2\theta + \theta^2) - (2 - \theta -
\theta^2)x_4
$$
$$
\iff [x_1 + x_2 + x_3 + x_4]\theta^2 + [x_4 + 2x_2 + 2x_3 - x_1]\theta - 2x_4 =
0
$$

Substituting the oberved data, we obtain
$197\theta^2 - 16\theta - 70 = 0$.
Using the quadratic formula, we solve and observe
$\hat{\theta} = 0.6381$.

## 1.b
$$
Q(\theta,\theta^{(k)}) = E_{\theta^{(k)}} [l_c(\theta)|\mathbf{X}]
= E_{\theta^{(k)}} [(x_{12} + x_4) \log \theta + (x_2 +
    x_3)\log(1-\theta)|\mathbf{X}]
$$
$$
= (E_{\theta^{(k)}} [x_{12}|\mathbf{X}] + x_4) \log \theta + (x_2 +
    x_3)\log(1-\theta)
= \left[ x_1 \left( \frac{\theta^{(k)}/4}{1/2 + \theta^{(k)}/4} \right)  + x_4
\right] \log \theta + (x_2 + x_3)\log(1-\theta)
$$

Let $b = \left( \frac{\theta^{(k)}/4}{1/2 + \theta^{(k)}/4} \right)$.
Differentiating and setting equal to $0$, we obtain
$$
\frac{x_1b  + x_4}{\theta} - \frac{(x_2 + x_3)}{1-\theta} = 0
$$
$$
\iff \theta(x_2 + x_3) = (x_1b + x_4)(1-\theta)
$$
$$
\iff \theta(x_2 + x_3 + x_1b + x_4) = x_1b + x_4
$$
$$
\iff \theta^{(k+1)} = \frac{x_1b + x_4}{x_2 + x_3 + x_1b + x_4}
$$

```{r}
expr <- function(theta) { ((1/4)*theta)/(1/2 + (1/4)*theta) }
B <- 100
x <- c(125,18,19,35)
thetas <- seq(0,1,by=0.25)
em.1 <- function(theta) {
    for(i in 1:B) {
        theta <- (x[1]*expr(theta) + x[4])/(x[2] + x[3] + x[1]*expr(theta) + x[4])
    }
  theta
}
sapply(thetas,em.1)
```

## 1.c

For completeness, we used $5$ starting values for $\theta: 0, 0.25, 0.5, 0.75,
1$. All of them yielded the same solution, $\hat{\theta} = 0.638$, which is the
same as the MLE obtained in part (a).

# 2
To derive the EM algorithm, we start with the likelihood:

$$
L_c(\alpha) = \prod_{i=1}^n \frac{2y_i}{\alpha^2} \exp
\left(-\frac{y_i^2}{\alpha^2} \right)
= \frac{2^n \prod_{i=1}^n y_i}{\alpha^{2n}} \exp \left( -\frac{1}{\alpha^2}
\sum_{i=1}^n y_i^2 \right)
$$

Then the log-likelihood:
$$
l_c(\alpha) = -2n \log \alpha - \frac{1}{\alpha^2} \sum_{i=1}^n y_i^2 + c
$$

Now we need an expression for $Q(\alpha,\alpha_{(k)})$:
$$
Q(\alpha,\alpha_{(k)}) = \mathbb{E}_{\alpha_{(k)}} [l_c(\alpha)|\mathbf{X}]
= -2n \log \alpha - \frac{1}{\alpha^2} \sum_{i=1}^n \mathbb{E}_{\alpha_{(k)}}
[y_i^2|\mathbf{X}]
$$

For the complete data we have simply that
$$
\mathbb{E}_{\alpha_{(k)}} [y_i^2|\mathbf{X}] = \mathbb{E}_{\alpha_{(k)}}
[y_i^2|y_i=y_i] = y_i^2
$$

For the censored data we must integrate:
$$
\mathbb{E}_{\alpha_{(k)}} [y_i^2|\mathbf{X}] = \mathbb{E}_{\alpha_{(k)}}
[y_i^2|y_i > c_i]
= \int_{c_i}^{\infty} \frac{2y_i^3}{\alpha_{(k)}^2} \exp \left(
-\frac{y_i^2}{\alpha_{(k)}^2} \right) d{y_i}
= (\alpha_{(k)}^2 + c_i^2) \exp \left( -\frac{c_i^2}{\alpha^2_{(k)}} \right)
$$

Let $K = \sum_{i=1}^n \mathbb{E}_{\alpha^{(k)}} [y_i^2|\mathbf{X}]$. We have
overall that:
$$
K = \sum_{i=1}^r y_i^2 + \sum_{i=r+1}^n (\alpha^2_{(k)} + c_i^2) \exp \left(
-\frac{c_i^2}{\alpha^2_{(k)}} \right)
$$

For the M-step, we maximize $Q(\alpha,\alpha_{(k)})$:
$$
Q(\alpha,\alpha_{(k)}) =  -2n \log \alpha - \frac{1}{\alpha^2} M
\frac{dQ}{d\alpha} = -\frac{2n}{\alpha} + \frac{2}{\alpha^3} M = 0
$$

Solving for $\alpha$, we obtain $\alpha_{(k+1)} = \sqrt{K/n}$, where $K$ is as
above.

Running the algorithm for $100$ iterations with $10$ distinct starting points
$\alpha^{(0)} = 1, 2, ..., 10$, we obtain $\hat{\alpha} = 0.0157576$ for each
distinct starting point.

# 3
$f(x)\propto e^{-x} \implies f(x) = \frac{1}{N}e^{-x}$ where
$N = (\int_{0}^{2}e^{-x}dx)^{-1} = \frac{1}{1-e^{-2}}$.
$$F(x) = \frac{1}{N}\int_{0}^{x}e^{-y}dy =
\frac{1-e^{-x}}{1-e^{-2}}1_{x\in[0,2]} $$

To sample from $F(x)$, we follow the algorithm outlined below:
```
### BEGIN: Pseudocode

u <- U(0,1) # Sample from a uniform distribution on the unit interval
find x such that F(x) == u
    invert F(x)
    target x = F^{-1}(u)

### END: Pseudocode
```
Inverting $F(x)$, we see
$$F(x) = u \implies \frac{1-e^{-x}}{1-e^{-2}} = u \implies
(1-e^{-2})u = 1-e^{-x} \implies x = -\log(1-u(1-e^{-2})) $$

```{r}
### BEGIN: Code
# R version 3.3.2
# Platform: x86_64-apple-darwin15.6.0 (64-bit)

## Imports
library(ggplot2) # pretty plotting

## Parameterize script
K = 500 # Number of samples
NN = (1 - exp(-2)) # Normalizing constant

## Suboutines
fx = function(x){
    # pdf for random variable X
    if(x<0){return(0)}
    if(x>2){return(0)}

    r = exp(-x) / NN
    return(r)
}

Fx = function(x){
    # cdf for random variable X
    if(x<0){return(0)}
    if(x>2){return(0)}

    r = (1 - exp(-x)) / NN
    return(r)
}

Fu = function(u){
    # Inverse cdf for the random variable X
    if(u<0){return(NaN)}
    if(u>1){return(NaN)}

    r = -log(1 - u*(1-exp(-2)))
    return(r)
}

## Simulate
Fuv = Vectorize(Fu) # Make Fu take vector arguments cleanly
us = runif(K, 0, 1) # K samples from U(0,1)
inv_xs = Fuv(us) # Inverse sampling method

## Plot results
# Simulated data with true distribution overlay
plt_xs = seq(0, 2, length.out = 100) # xs for true fx(x) pdf
fxv = Vectorize(fx)
plt_ys = fxv(plt_xs) # true fx(x) = ys

df_tru = data.frame(x_tru = plt_xs, y_tru = plt_ys)
df_sim = data.frame(x_sim = inv_xs)

plt1 = ggplot() +
    geom_histogram( # Simulated distribution
                data = df_sim,
                aes(x = x_sim, y = ..density..),
                binwidth = 0.05,
                color="royalblue",
                fill="slategray3"
            ) +
    geom_line( # True distribution
                data = df_tru,
                aes(x = x_tru, y = y_tru),
                color="deeppink2"
            ) +
    labs(
                title = "True PDF and inverse samples",
                x = "X",
                y = "Density"
            )
plt1

### END: Code
```
The histogram represents the density of inverse sampled $X$'s from the
simulation. The red line represents the true distribution for reference.


# 4
First we need to get our sampling distribution in order. This consists of
finding the normalization constant $N$ as follows:
$f(x) = \frac{1}{N}\frac{e^{-x}}{1+x^2}$ so
$\int_0^{\infty} f(x)dx = 1 \implies
N = \int_{0}^{\infty} q(x)dx \approx 0.6214496$.

Now we need to find the envelope constants $M_i$ for $i\in\{0,1\}$.
  
First, we find $M_1$ such that $f(x)\le M_1g_1(x)$ for all $x>0$:
$f(x) = \frac{1}{N}q(x) = \frac{1}{N}\frac{e^{-x}}{1+x^2} \le
M_1g_1(x) = M_1e^{-x}$ so
$NM_1 \ge \frac{1}{1+x^2} \implies
M_1 = \frac{1}{N} \sup_{x>0} \frac{1}{1+x^2} \implies
M_1 = \frac{1}{N}$
  
Next, we find $M_2$ such that $f(x)\le M_2g_2(x)$ for all $x>0$:
$\frac{1}{N}\frac{e^{-x}}{1+x^2} \le
M_2g_2(x) = M_2\frac{2}{\pi(1+x^2)}$ so
$\frac{e^{-x}}{N} \le \frac{2M_2}{\pi} \implies
M_2 \ge \frac{\pi e^{-x}}{N} \implies
M_2 = \sup_{x>0}\frac{\pi e^{-x}}{N} = \frac{\pi}{N}
$
  
TODO: insert code for this problem
```{r}
### BEGIN: Code

## Imports
library(reshape)

## Parameterize
K = 500
NN = 0.6214496 # normalizing constant for fx(x) the pdf
M1 = 1/NN # Resampling constant ensuring fx(x) <= M*g1(x) for all x
M2 = pi/NN

## BEGIN: Subroutines

fx = Vectorize(function(x){
    # pdf for problem 4
    if(x<0){return(0)}
    r = exp(-x) / (1+x^2) / NN
    return(r)
})

g1 = Vectorize(function(x){
    # First envelope density for rejection sampling
    if(x<0){return(0)}
    r = exp(-x)
    return(r)
})

g2 = Vectorize(function(x){
    # Second envelope density for rejection sampling
    if(x<0){return(0)}
    r = 2/(pi*(1+x^2))
    return(r)
})

## END: Subroutines

## BEGIN: Simulate

# First, g1
N = 200 # resolution of the discretized x axis
xdisc = seq(0,5,length.out=N) # discretization of the x axis
g1i = 1 # how many loops this envelope rejection sampling takes
i = 1 # list counter
g1rs = rep(NA, K) # initialize empty result vector
while(i <= K){
    x = sample(xdisc, 1, prob=g1(xdisc)) # proposition from envelope
    u = runif(1)
    w = fx(x)/(M1*g1(x))
    if(w>u){
        # Accept sample
        g1rs[i] = x
        i = i + 1
    }
    g1i = g1i + 1
}

# Now g2
g2i = 1 # how many loops this envelope rejection sampling takes
i = 1 # list counter
g2rs = rep(NA, K) # initialize empty result vector
while(i <= K){
    x = sample(xdisc, 1, prob=g2(xdisc)) # proposition from envelope
    u = runif(1)
    w = fx(x)/(M2*g2(x))
    if(w>u){
        # Accept sample
        g2rs[i] = x
        i = i + 1
    }
    g2i = g2i + 1
}

## END: Simulate

## Plot
# simulation results
true_df = data.frame(x=xdisc,
                     g1=g1(xdisc),
                     g2=g2(xdisc),
                     f=fx(xdisc)
                     )
true_df_m = melt(true_df, id="x")
names(true_df_m) = c("x", "Distribution", "y")

plt3 = ggplot() +
    geom_line(data=true_df_m,
              aes(x=x, y=y, color=Distribution)) +
    labs(title="Target distribution and envelopes",
         x="X", y="Probability")
plt3

gsim_df = data.frame(g1=g1rs, g2=g2rs)
plt4 = ggplot() + # g1 simulation results histogram
    geom_histogram(data=gsim_df,
                   mapping=aes(x=g1,y=..density..),
                   binwidth=0.08) +
    geom_line(data=true_df_m[true_df_m$Distribution != "g2",],
              aes(x=x, y=y, color=Distribution)) +
    labs(title="Envelope sampling from g1",
         x="X", y="Frequency")
plt4

plt5 = ggplot() + # g2 simulation results histogram
    geom_histogram(data=gsim_df,
                   mapping=aes(x=g2,y=..density..),
                   binwidth=0.08) +
    geom_line(data=true_df_m[true_df_m$Distribution != "g1",],
              aes(x=x, y=y, color=Distribution)) +
    labs(title="Envelope sampling from g2",
         x="X", y="Frequency")
plt5

### END: Code
```
The envelope sampling simulation results are similar for both $g_1(x)$ and
$g_2(x)$ envelopes. However, using envelope $g_1(x)$ took only
$`r g1i`$ propositions to produce $`r K`$ samples compared to the
$`r g2i`$ propositions required when using envelope $g_2(x)$.

# 5
The algorithm for sampling from $f(x,y) = \frac{1}{N} q(x,y)$ for
$0<x<1$, $0<y<1$, and $x^2+y^2\le1$ where $q(x,y) = x^\alpha y$ follows a brief
derivation of the normalizing constant $N$ for $f(x,y)$:

$$
1 = \int_0^{1} \int_0^{\sqrt{1-x^2}} f(x,y) dydx =
\int_0^{1} \int_0^{\sqrt{1-x^2}} \frac{1}{N}x^\alpha y dydx =
\frac{1}{N} \int_0^{1} \frac{x^\alpha y^2}{2}
    \bigg\rvert_{y=0}^{\sqrt{1-x^2}} dx =
$$

$$
\frac{1}{N} \int_0^{1} \frac{x^\alpha (1-x^2)}{2} dx =
\frac{1}{2N} (
    \frac{x^{\alpha+1}}{\alpha+1} -
    \frac{x^{\alpha+3}}{\alpha+3}
    ) \bigg\rvert_{x=0}^{1} =
\frac{1}{2N}(\frac{1}{\alpha+1}-\frac{1}{\alpha+3}) \implies
N = \frac{1}{2}(\frac{1}{\alpha+1}-\frac{1}{\alpha+3})
$$

Inverse sampling won't work because $F^{-1}(u)$ doesn't map to a unique pair
$(x,y)$. We could induce an ordering on a discretized grid using the CDF, but
this introduces yet another tuning parameter namely the grid resolution. A
potential algorithm inspired by the Gibbs Sampling algorithm is to sample from
the marginal of either $X$ or $Y$, then condition on that sampled value to
obtain a univariate conditional distributon e.g. $F(X\mid Y)$ and obtain the
sample for the other from that. Whether $X$ or $Y$ is sampled first is decided
by a $B(1/2)$ sample. This Gibbs-like algorithm follows:

```
K <- desired sample size
rs <- rep(NA, K, double)
i <- 1
while length(rs != NA) < K do:
    u ~ U(0,1)
    w = (u>1/2)

    if w then:
        x ~ F_{X}(x)
        y ~ F_{Y|X}(y|x)
    else:
        y ~ F_{Y}(y)
        x ~ F_{X|Y}(x|y)

    rs[i] = (x,y)
    i = i + 1

return(rs)
```

Samples from the marginal and conditional distributions above are performed
using inversion sampling on their respective CDFs. Notice that, because the
second sample is taken from the conditional, there is no rejection step required
- samples that would be rejected are a priori rendered impossible by the
conditional distribution. Further, note that this algorithm takes only K loops
to complete.

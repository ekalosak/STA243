---
title: "STA243 Hw6"
author:
- Eric Kalosa-Kenyon
date: "`r format(Sys.time(), '%d %B %Y')`"

abstract: In this assignment, I examine some basics of bootstrapping.

output:
  html_document:
    toc: yes
---

# Comparison of bootstrap estimate with analytical results
In this section, I calculate the analytical distribution of a maximum likelihood
estimator and compare it to bootstrapped estimates thereof.

$$
X_1,\dots,X_n \sim{\rm iid} U(0,\theta) \implies
\hat{\theta}_{MLE} = {\rm max}X_i
$$

## Analytical distribution of $\hat{\theta}$
Recall that the PDF for order statistics is
$$
f_{X_{(k)}}(x)=
\frac{n!}{(k-1)!(n-k)!} (F_{X}(x))^{k-1}
(1-F_{X}(x))^{n-k}f_{X}(x)
\quad \quad {\rm [1]}
$$

where $F_X(x)=\frac{x}{\theta}$ is the CDF of $X$ and $f_X(x)=\frac{1}{\theta}$
is the PDF of $X$.
The MLE $\hat{\theta}=X_{(n)}$ is the maximum, so we are interested in the above
PDF when $k=n$:
$$
f_{X_{(n)}}(x) =
n (F_{X}(x))^{n-1} f_{X}(x) =
\frac{n}{\theta} (\frac{x}{\theta})^{n-1}
$$

## Variance of $\hat{\theta}$
I am interested in finding the variance of the MLE $\hat{\theta}$.
$$
{\rm var}(\hat{\theta}) = {\rm E}(\hat{\theta}^2) - {\rm E}(\hat{\theta})^2
\quad \quad [2]
$$

$$
{\rm E}_\theta(\hat{\theta}) = \int_0^\theta x f_{X_{(n)}}(x) dx =
\int_0^\theta xn \left(\frac{x}{\theta}\right)^{n-1} \frac{1}{\theta} dx =
\frac{n}{n+1}\theta
$$

$$
{\rm E}_\theta(\hat{\theta}^2) = \int_0^\theta x^2 f_{X_{(n)}}(x) dx =
\int_0^\theta x^2n \left(\frac{x}{\theta}\right)^{n-1} \frac{1}{\theta} dx =
\frac{n}{n+2}\theta^2
$$

$$
{\rm var}(\hat{\theta}) =
\frac{n}{n+2}\theta^2 - \left(\frac{n}{n+1}\theta\right)^2 =
n\theta^2\left(\frac{1}{n+2} - \frac{n}{(n+1)^2}\right)
$$

Let ${\rm Var}_{F_\theta}(\hat{\theta}) \triangleq
{\rm var}(\hat{\theta})$ as defined above.

## Parametric bootstrapping ${\rm Var}_{F_\theta}(\hat{\theta})$
In this section, I generate a dataset of size $n = 50$ from the distribution
$X_i\sim{\rm iid}U(0,3)$ and use it to approximate
${\rm Var}_{F_\theta}(\hat{\theta})$ using $B=5000$ parametric bootstrap
samples. The difference between parametric and nonparametric bootstrap is that
in the parametric bootstrap I generate samples from $U(0,\hat{\theta})$ and in
the nonparametric bootstrap I generate samples from the emperical distribution.
In this section, I perform the parametric bootstrap.

```{r}
## Preamble
set.seed(33)

## Import external libraries
library(ggplot2)
library(latex2exp)

## Parameterize
n = 50
t = 3
B = 5000

## Generate original dataset
xs = runif(n, 0, t)

## Generate parametric bootstrap samples and calculate the sample statistic
## variance

# generate samples
that = max(xs)
bs_param_raw = runif(n*B, 0, that)
bs_param = matrix(bs_param_raw, nrow=B, ncol=n)

# calculate max of each sample
maxs_param = apply(bs_param, 1, max) # 1 means apply to rows, 2 would be columns

# calculate the parametric bootstrapped variance of \hat{\theta}
var_boot_param = var(maxs_param)

## Calculate the analytical variance
var_anal = n*t^2*(1/(n+2) - n/(n+1)^2)

## Plot parametric bootstrap samples
plt_var_param = ggplot(data=data.frame(x=maxs_param), aes(x=x)) +
    geom_histogram(color="steelblue", binwidth=0.025) +
    # geom_vline(aes(xintercept=t), size=1.5, color="coral") +
    labs(
        x=TeX("Resampled estimate of $\\hat{\\theta}$"),
        y="Frequency",
        title=
        TeX("Parametric bootstrapping to estimate Var($\\hat{\\theta}$)")
    )
```

```{r echo=F}
plt_var_param
```
  
The parametric bootstrapped variance of $\hat{\theta}$ is $`r var_boot_param`$
and the analytical variance thereof is $`r var_anal`$.
The log-ratio of these two values is $`r log(var_boot_param/var_anal)`$,
suggesting that the parametric bootstrapped estimate is relatively accurate in
this case.

## Nonparametric bootstrapping the same quantity
In this section, I perform the nonparametric bootstrap to estimate the variance
of the MLE $\hat{\theta}={\rm max}(\{X_1,\dots,X_n\})$.

```{r}
## Generate nonparametric bootstrap samples and calculate var(\hat{\theta})

# generate bootstrap
bs_nonparam_raw = sample(xs, size=n*B, replace=TRUE)
bs_nonparam = matrix(data=bs_nonparam_raw, nrow=B, ncol=n)

# calculate max of each bootstrap sample
maxs_nonparam = apply(bs_nonparam, 1, max)

# calculate nonparametric bootstrapped variance of the mle
var_boot_nonparam = var(maxs_nonparam)

## Plot variances
plt_var_nonparam = ggplot(data=data.frame(x=maxs_nonparam), aes(x=x)) +
    geom_histogram(color="steelblue", binwidth=0.025) +
    # geom_vline(aes(xintercept=t), size=1.5, color="coral") +
    labs(
        x=TeX("Resampled estimate of $\\hat{\\theta}$"),
        y="Frequency",
        title=
        TeX("Nonparametric bootstrapping to estimate Var($\\hat{\\theta}$)")
    )

### END: Code
```

```{r echo=F}
plt_var_nonparam
```
  
The nonparametric bootstrapped variance of
$\hat{\theta}$ is $`r var_boot_nonparam`$ and the analytical variance thereof is
$`r var_anal`$.  The log-ratio of these two values is
$`r log(var_boot_nonparam/var_anal)`$, suggesting that the bootstrapped estimate
is somewhat accurate in this case.

## Comparison of parametric and nonparametric bootstrapping
In the case where $X_i\sim{\rm iid}U(0,\theta)$, parametric bootstrap estimates
of the the variance of the MLE estimator
$\hat{\theta} \triangleq {\rm max}(\{X_1,\dots,X_n\})$ are more accurate than
their nonparametric counterparts.

$$
\frac{|{\rm Var}_{F_\theta}(\hat{\theta}) - {\rm Var}_{p}(\hat{\theta})|}
{|{\rm Var}_{F_\theta}(\hat{\theta}) - {\rm Var}_{np}(\hat{\theta})|} =
\frac{`r abs(var_anal - var_boot_param)`}
{`r abs(var_anal - var_boot_nonparam)`} \approx
`r abs(var_anal - var_boot_param)/abs(var_anal - var_boot_nonparam)`
$$
That this ratio is less than $1$ provides evidence for the claim that the
parametric bootstrap is more accurate in this case. At a glance, the
nonparametric bootstrap appears to underestimate the actual variance of the MLE
$\hat{\theta}$. One reason for this appears to be that the efficiency of the
nonparametric bootstrap is tied strongly to the sample size than the parametric
bootstrap, but this would require a technical proof to verify.

# Bootstrapping regression coefficients
In this section, I examine bootstrapping as a method to estimate confidence
intervals on regression coefficients.

## Generating observations
I consider two models and generate $n=512$ observations from each. The first
model is described by the following algorithm, additional source code found in
refference $[3]$.

```{r warning=FALSE, message=FALSE}
foo = "bar"
```

```
## Import external source
source("genetic_supplement.R")

## Parameterize
n = 512

## Generate observations
x1s = (0:(n-1))/n
fx1s = model1(x1s)
y1s = fx1s + noise1(x1s)

## Plot generated observations
plt_mod1 = ggplot(data=data.frame(x=x1s, y=y1s, f=fx1s)) +
    geom_point(aes(x=x, y=y), color="steelblue", alpha=0.8) +
    geom_line(aes(x=x, y=f), color="darkorange1", size=1.1, alpha=0.5) +
    labs(x="X", y="Y", title="Model 1")
```

```{r echo=F}
foo = "bar"
```

```
plt_mod1
```

The second model is
$$f(x) = (4x − 2) + 2 e^{−16(4x − 2)^2}$$

```{r}
foo = "bar"
```

```
## Subroutines
model2 = function(x){
    r = (4*x-2) + 2*exp(-16*(4*x-2)^2)
    return(r)
}

## Generate observations
x2s = (0:(n-1))/n
fx2s = model2(x2s)
y2s = fx2s + rnorm(n, sd=(fx2s^2/5))

## Plot generated observations
plt_mod2 = ggplot(data=data.frame(x=x2s, y=y2s, f=fx2s)) +
    geom_point(aes(x=x, y=y), color="steelblue", alpha=0.8) +
    geom_line(aes(x=x, y=f), color="darkorchid", size=1.1, alpha=0.5) +
    labs(x="X", y="Y", title="Model 2")
```

```{r echo=F}
foo = "bar"
```

```
plt_mod2
```

## Genetic algorithm for calculating regression curve estimates
In this section, I use a genetic algorithm to calculate piecewise constant
regression curves for the two models illustrated in the preceeding section. All
genetic source code is contained in refference [3].
First, I regress for the naturally piecewise constant model using the AIC
fitness criterion.

```{r warning=FALSE}
foo = "bar"
```

```
## Parameterize
G = 30 # generations
K = 10 # individuals per generation
Pcross = 0.7 # Crossover rate
Pmutate = 0.08 # Mutation rate
Pimmigrate = 0.3 # Rate of new individual introduction into population
Psex = 0.4 # Rate of sexual vs asexual reproduction
which_penalty1 = "AIC" # "AIC" or "MDL"

## Simulate
pop1 = genetic_piecewise_regression(y1s, x1s, G, K,
                                   Pcross, Pmutate, Pimmigrate, Psex,
                                   which_penalty1)

## Plot results
plot_df1 = data.frame(y=y1s, x=1:n)

best_organism1 = get_best_org(pop1, which_penalty1)
best_score1 = best_organism1$AIC # NOTE: use an if/else block here
best_chromosome1 = best_organism1$Chromosome

plt_best1 = plot_chromosome(best_chromosome1, plot_df1, color="gold") +
    ggtitle(
        paste(
            "Chromosome ", rownames(best_organism1), " in generation ",
            best_organism1$Generation,
            " with ", which_penalty1, "=", best_score1,
            sep=""
        ))
```

```{r echo=F}
foo = "bar"
```

```
plt_best1
```
  
Second, I regress the smoother model from the preceeding section using the MDL
fitness criterion.

```{r warning=FALSE}
foo = "bar"
```

```
## Parameterize
G = 30 # generations
K = 10 # individuals per generation
Pcross = 0.7 # Crossover rate
Pmutate = 0.18 # Mutation rate
Pimmigrate = 0.6 # Rate of new individual introduction into population
Psex = 0.2 # Rate of sexual vs asexual reproduction
which_penalty2 = "MDL" # "AIC" or "MDL"

## Simulate
pop2 = genetic_piecewise_regression(y2s, x1s, G, K,
                                   Pcross, Pmutate, Pimmigrate, Psex,
                                   which_penalty2)

## Plot results
plot_df2 = data.frame(y=y2s, x=1:n)

best_organism2 = get_best_org(pop2, which_penalty2)
best_score2 = best_organism2$AIC # NOTE: use an if/else block here
best_chromosome2 = best_organism2$Chromosome

plt_best2 = plot_chromosome(best_chromosome2, plot_df2, color="tomato1") +
    ggtitle(
        paste(
            "Chromosome ", rownames(best_organism2), " in generation ",
            best_organism2$Generation,
            " with ", which_penalty2, "=", best_score2,
            sep=""
        ))
```

```{r echo=F}
foo = "bar"
```

```
plt_best2
```

## Bootstrapped 95% confidence bands for regression curves
### Bootstrapping residuals
Here I construct 95% pointwise confidence bands for both curves using residual
bootstrapping. First, I'll bootstrap the residuals of the naturally piecewise
constant function

```{r}
foo = "bar"
```

```
# get residuals
# for each boot
#   sample n residuals w repl
#   add these to regressed y
#   use this new sample to compute another regression
#   record regressed values at each point
#   these serve as the emperical distribution for each point

## Parameterize
B = 10 # number of bootstraps

bootstrap_residuals = function(B, best_chromosome, ys, xs, which_penalty){

    ## Recover residuals
    yhats = regress_chromosome(best_chromosome, ys, xs)
    res = ys - yhats

    ## Bootstrap residuals
    n = length(ys)
    point_ests = data.frame(matrix(nrow=B, ncol=n))
    for(b in 1:B){
        # get residuals and make y* = y + e*
        boot_resid = base::sample(res, n)
        boot_ys = ys + boot_resid
        # refit the model i.e. run another genetic simulation
        boot_pop = genetic_piecewise_regression(boot_ys, xs,
            G, K, Pcross, Pmutate, Pimmigrate, Psex, which_penalty)
        boot_best_org = get_best_org(boot_pop, which_penalty)
        boot_best_chrom = boot_best_org$Chromosome
        boot_yhats = regress_chromosome(boot_best_chrom, boot_ys, xs)
        # record the fitted values
        point_ests[b,] = boot_yhats
    }

    return(point_ests)
}

## Recover the bootstrapped pointwise quantiles
point_ests1 = bootstrap_residuals(B, best_chromosome1, y1s, x1s, which_penalty1)
y1hats = regress_chromosome(best_chromosome1, y1s, x1s)
boot_quantiles1 = sapply(
        point_ests1,
        (function(x) quantile(x, probs=c(0.025, 0.975)))
    )

## Plot the pointwise confidence intervals
df_95c_resid1 = data.frame(cbind(t(boot_quantiles1), y1hats, y1s, x1s))
colnames(df_95c_resid1) = c("low", "up", "y", "yt", "x")

plt_boot_resid1 = ggplot(data=df_95c_resid1) +
    geom_ribbon(aes(x=x, ymin=low, ymax=up),
                color="steelblue", alpha=0.6) +
    geom_line(aes(x=x, y=y), color="gold", size=1.1) +
    geom_point(aes(x=x, y=yt),
               color="steelblue", alpha=0.5, size=0.8) +
    geom_line(data=data.frame(x=x1s, y=fx1s),
              aes(x=x, y=y), color="darkorange1", size=1.1) +
    labs(x="x", y="y", title="Residual bootstrapped 95% confidence band")
```

```{r echo=F}
foo = "bar"
```

```
plt_boot_resid1
```
  
The 95% confidence bands are very tight around the jumps. This is a particularly
impressive quality of this bootstrapped confidence band.

Next, I perform the residual bootstrap on the smooth model to obtain a pointwise
95% confidence band.

```{r}
foo = "bar"
```

```
## Parameterize
B = 10 # number of bootstraps

## Recover the bootstrapped pointwise quantiles
point_ests2 = bootstrap_residuals(B, best_chromosome2, y2s, x2s, which_penalty2)
y2hats = regress_chromosome(best_chromosome2, y2s, x1s)
boot_quantiles2 = sapply(
        point_ests2,
        (function(x) quantile(x, probs=c(0.025, 0.975)))
    )

## Plot the pointwise confidence intervals
df_95c_resid2 = data.frame(cbind(t(boot_quantiles2), y2hats, y2s, x2s))
colnames(df_95c_resid2) = c("low", "up", "y", "yt", "x")
plt_boot_resid2 = ggplot(data=df_95c_resid2) +
    geom_ribbon(aes(x=x, ymin=low, ymax=up),
                color="steelblue", alpha=0.6) +
    geom_line(aes(x=x, y=y), color="tomato1", size=1.1) +
    geom_point(aes(x=x, y=yt),
               color="steelblue", alpha=0.5, size=0.8) +
    geom_line(data=data.frame(x=x2s, y=fx2s),
              aes(x=x, y=y), color="darkorchid", size=1.1) +
    labs(x="x", y="y", title="Residual bootstrapped 95% confidence band")
```

```{r echo=F}
foo = "bar"
```

```
plt_boot_resid2
```

### Bootstrapping pairs
In this section, I perform different kind of bootstrapping to estimate the
pointwise 95% confidence band for both models. This second bootstrapping method
is much more straightforward: I simply resample the raw observations and use
them to fit new curves whose fitted values serve as my pointwise emperical
estimates. I use the pointwise estimates as quantiles to estimate a 95%
confidence band.

```{r}
foo = "bar"
```

```
# get xs and ys
# for each boot
#   sample n (x,y) pairs
#   use this new sample to compute another regression
#   record regressed values at each point
#   these serve as the emperical distribution for each point

## Parameterize
B = 10 # number of bootstraps

bootstrap_pairs = function(B, best_chromosome, ys, xs, which_penalty){

    yhats = regress_chromosome(best_chromosome, ys, xs)

    ## Bootstrap pairs
    n = length(ys)
    point_ests = data.frame(matrix(nrow=B, ncol=n))
    for(b in 1:B){
        # sample n pairs of xs and ys
        boot_ixs = sample(x=1:n, size=n, replace=T)
        boot_ys = ys[boot_ixs]
        boot_xs = xs[boot_ixs]
        # regress to these sampled pairs using genetic algorithm p.w-constant
        boot_pop = genetic_piecewise_regression(boot_ys, boot_xs,
            G, K, Pcross, Pmutate, Pimmigrate, Psex, which_penalty)
        boot_best_org = get_best_org(boot_pop, which_penalty)
        boot_best_chrom = boot_best_org$Chromosome
        # calculate and record the regression estimates
        boot_yhats = regress_chromosome(boot_best_chrom, boot_ys, boot_xs)
        point_ests[b,] = boot_yhats
    }

    return(point_ests)
}

## Recover the bootstrapped pointwise quantiles
point_ests1 = bootstrap_pairs(B, best_chromosome1, y1s, x1s, which_penalty1)
y1hats = regress_chromosome(best_chromosome1, y1s, x1s)
boot_quantiles1 = sapply(
        point_ests1,
        (function(x) quantile(x, probs=c(0.025, 0.975)))
    )

## Plot the pointwise confidence intervals
df_95c_pair1 = data.frame(cbind(t(boot_quantiles1), y1hats, y1s, x1s))
colnames(df_95c_pair1) = c("low", "up", "y", "yt", "x")

plt_boot_pair1 = ggplot(data=df_95c_pair1) +
    geom_ribbon(aes(x=x, ymin=low, ymax=up),
                color="steelblue", alpha=0.6) +
    geom_line(aes(x=x, y=y), color="gold", size=1.1) +
    geom_point(aes(x=x, y=yt),
               color="steelblue", alpha=0.5, size=0.8) +
    geom_line(data=data.frame(x=x1s, y=fx1s),
              aes(x=x, y=y), color="darkorange1", size=1.1) +
    labs(x="x", y="y", title="Pair bootstrapped 95% confidence band")
```

```{r echo=F}
foo = "bar"
```

```
plt_boot_pair1
```

With a similar number of bootstrap samples, this confidence band is clearly less
tight than the previously exhibited residual bootstrap for the same purpose.

Next, I examine the pair bootstrap on the smooth model.

```{r}
foo = "bar"
```

```
## Parameterize
B = 10 # number of bootstraps

## Recover the bootstrapped pointwise quantiles
point_ests2 = bootstrap_pairs(B, best_chromosome2, y2s, x2s, which_penalty2)
y2hats = regress_chromosome(best_chromosome2, y2s, x2s)
boot_quantiles2 = sapply(
        point_ests2,
        (function(x) quantile(x, probs=c(0.025, 0.975)))
    )

## Plot the pointwise confidence intervals
df_95c_pair2 = data.frame(cbind(t(boot_quantiles2), y2hats, y2s, x2s))
colnames(df_95c_pair2) = c("low", "up", "y", "yt", "x")

plt_boot_pair2 = ggplot(data=df_95c_pair2) +
    geom_ribbon(aes(x=x, ymin=low, ymax=up),
                color="steelblue", alpha=0.6) +
    geom_line(aes(x=x, y=y), color="tomato1", size=1.1) +
    geom_point(aes(x=x, y=yt),
               color="steelblue", alpha=0.5, size=0.8) +
    geom_line(data=data.frame(x=x2s, y=fx2s),
              aes(x=x, y=y), color="darkorchid", size=1.1) +
    labs(x="x", y="y", title="Pair bootstrapped 95% confidence band")
```

```{r echo=F}
foo = "bar"
```

```
plt_boot_pair2
```
  
It looks like the pair bootstrap performs worse on both models when compared to
the residual bootstrap.

## Pseudocode for estimating jump point locations
As the reader may observe, there are severe discontinuities in one of the models
considered in this presentation. These are indeed points of interest and here I
provide the outline of an algorithm that uses bootstrapping to detect the
locations of these jump points.

```
Algorithm: Bootstrapped Jump Point Detection
In: X = (X1, .., XN); Y = (Y1, .., YN); B
Out: C = (Xc1, .., XcK)

mod = calculate regression model Y ~ X
yh = predict model output over X
res = calculate residuals using Y and yh
n =  the number of observations

mods = empty list
for b in 1:B do
    boot_res = sample n residuals with replacement
    boot_Y = Y + boot_res
    boot_mod = calculate regression model boot_Y ~ X
    append boot_mod to mods

pr_jump = empty vector of length n-1 i.e. the length of the regression model
for i in 1:length(mod) do
    pr_jump[i] = the proportion of bootstrapped models in mods that indicate a
                jump at position i

plot pr_jump
```

# Estimating quantile range with the jackknife
In this section, I use the bootstrap and its predesessor, the jackknife, to
simulate confidence intervals for $\theta$.

$$
\theta \triangleq \frac{q_{0.75}-q_{0.25}}{1.34} \quad {\rm for} \quad
X_1,\dots,X_n \sim {\rm iid} \, t_{df} \quad {\rm using} \quad
n = 25 \, {\rm and} \, df=3
$$

```{r}
q1 = qt(0.75, df=3)
q0 = qt(0.25, df=3)
theta_tr = (q1 - q0) / 1.34
```

The true value of $\theta$ is
$\frac{`r q1`-(`r q0`)}{1.34} \approx `r theta_tr`$.

## Generating observations
```{r}
## Parameterize
n = 25
df = 3
xs = rt(n=n, df=df)
q0 = 0.25
q1 = 0.75

## Subroutines
theta = function(xs_){
    q0_ = quantile(xs_, q0)
    q1_ = quantile(xs_, q1)
    t_ = (q1_ - q0_)/1.34
    return(t_)
}

## Estimate statistic using jackknife
# normal theory w se from jack
jts = c()
for(i in 1:n){
    jxs = xs[1:n != i]
    jt = theta(jxs)
    jts = c(jts, jt)
}
sej = sqrt(((n-1)/n) * sum((jts - mean(jts))^2))

njci = theta(xs) + c(-1,1)*1.645*sej
```
The normal theory jackknife 90% confidence interval estimate for $\theta$ is
$\hat{\theta}_{J} = `r theta(xs)` \pm `r 1.645*sej` \approx [`r njci`]$.

Next I perform the normal theory bootstrap to generate another 90% confidence
interval.

```{r}
## Parameterize
B = 1000

## Subroutines
oneboot = function(xs_, stat_){
    n = length(xs_)
    samp = sample(xs_, n, replace=T)
    r = stat_(samp)
    return(r)
}

## Perform the bootstrap and calculate the confidence interval
boot_stats = replicate(B, oneboot(xs, theta))
boot_se = sqrt(sum((boot_stats - mean(boot_stats))^2)/(B-1))
boot_ci_norm = theta(xs) + c(-1,1)*1.645*boot_se
```
The normal theory bootstrapped 90% confidence interval estimate for $\theta$ is
$\hat{\theta}_{B} = `r theta(xs)` \pm
`r 1.645*boot_se` \approx [`r boot_ci_norm`]$.

Next, I calculate the semiparametric bootstrapped 90% confidence interval.

```{r}
zs = (boot_stats - theta(xs))/boot_se
boot_ts = quantile(zs, c(0.05, 0.95))
boot_semi_ci = theta(xs) + boot_ts*sqrt(var(xs)/n)
```
The semiparametric bootstrapped 90% confidence interval estimate for $\theta$ is
$\hat{\theta}_{B^\star} = [`r boot_semi_ci`]$.

Finally, I perform the completely nonparametric bootstrap to generate another
90% confidence interval.

```{r}
B = 2500
boot_stats_np = replicate(B, oneboot(xs, theta))
boot_np_ci = quantile(boot_stats_np, c(0.05, 0.95))
```
The nonparametric bootstrapped 90% confidence interval estimate for $\theta$ is
$\hat{\theta}_{B^{\star\star}} = [`r boot_np_ci`]$.

Following is a graph of the observations with the estimated $\hat{\theta}$
confidence intervals overlayed for convenience.

```{r echo=F}
## Plot
plt_ts = ggplot(data=data.frame(x=xs)) +
    geom_histogram(aes(x=x), color="steelblue", bins=30) +
    labs(x="X", y="Frequency", title="Histogram of generated observations") +
    geom_segment(aes(x = njci[1], y = 0.1, xend = njci[2], yend = 0.1),
                 color="tomato1", size=1.1) +
    geom_segment(aes(x = boot_ci_norm[1], y = 0.25,
                     xend = boot_ci_norm[2], yend = 0.25),
                 color="darkorchid", size=1.1) +
    geom_segment(aes(x = boot_semi_ci[1], y = 0.4,
                     xend = boot_semi_ci[2], yend = 0.4),
                 color="darkorange1", size=1.1) +
    geom_segment(aes(x = boot_np_ci[1], y = 0.55,
                     xend = boot_np_ci[2], yend = 0.55),
                 color="olivedrab", size=1.1) +
    geom_vline(xintercept = theta_tr, color="gold", size=1.1)

plt_ts
```
  
The golden vertical line depicts the true $\theta=`r theta_tr`$ calculated at
the beginning of this section. The red segment is the normal theory jackknife
90% confidence interval, the purple segment is the normal theory bootstrap 90%
confidence interval, the orange segment is the semiparametric bootstrap 90%
confidence interval, and the olive segment is the nonparametric 90% bootstrapped
confidence interval.

Notice that the semiparametric bootstrapped interval is accurate and the
tightest.

# Sources
1. https://en.wikipedia.org/wiki/Order_statistic
2. https://en.wikipedia.org/wiki/Variance
3. https://github.com/ekalosak/STA243/blob/master/hw6/genetic_supplement.R
4. https://en.wikipedia.org/wiki/Bootstrapping_(statistics)#Resampling_residuals

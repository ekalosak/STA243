---
title: "STA243 Hw6"
author:
- Eric Kalosa-Kenyon
date: "`r format(Sys.time(), '%d %B %Y')`"

abstract: In this assignment, I examine some basics of bootstrapping.

output:
  html_document:
    toc: yes
---

# Comparison of bootstrap estimate with analytical results
In this section, I calculate the analytical distribution of a maximum likelihood
estimator and compare it to bootstrapped estimates thereof.

$$
X_1,\dots,X_n \sim{\rm iid} U(0,\theta) \implies
\hat{\theta}_{MLE} = {\rm max}X_i
$$

## Analytical distribution of $\hat{\theta}$
Recall that the PDF for order statistics is
$$
f_{X_{(k)}}(x)=
\frac{n!}{(k-1)!(n-k)!} (F_{X}(x))^{k-1}
(1-F_{X}(x))^{n-k}f_{X}(x)
\quad \quad {\rm [1]}
$$

where $F_X(x)=\frac{x}{\theta}$ is the CDF of $X$ and $f_X(x)=\frac{1}{\theta}$
is the PDF of $X$.
The MLE $\hat{\theta}=X_{(n)}$ is the maximum, so we are interested in the above
PDF when $k=n$:
$$
f_{X_{(n)}}(x) =
n (F_{X}(x))^{n-1} f_{X}(x) =
\frac{n}{\theta} (\frac{x}{\theta})^{n-1}
$$

## Variance of $\hat{\theta}$
I am interested in finding the variance of the MLE $\hat{\theta}$.
$$
{\rm var}(\hat{\theta}) = {\rm E}(\hat{\theta}^2) - {\rm E}(\hat{\theta})^2
\quad \quad [2]
$$

$$
{\rm E}_\theta(\hat{\theta}) = \int_0^\theta x f_{X_{(n)}}(x) dx =
\int_0^\theta xn \left(\frac{x}{\theta}\right)^{n-1} \frac{1}{\theta} dx =
\frac{n}{n+1}\theta
$$

$$
{\rm E}_\theta(\hat{\theta}^2) = \int_0^\theta x^2 f_{X_{(n)}}(x) dx =
\int_0^\theta x^2n \left(\frac{x}{\theta}\right)^{n-1} \frac{1}{\theta} dx =
\frac{n}{n+2}\theta^2
$$

$$
{\rm var}(\hat{\theta}) =
\frac{n}{n+2}\theta^2 - \left(\frac{n}{n+1}\theta\right)^2 =
n\theta^2\left(\frac{1}{n+2} - \frac{n}{(n+1)^2}\right)
$$

Let ${\rm Var}_{F_\theta}(\hat{\theta}) \triangleq
{\rm var}(\hat{\theta})$ as defined above.

## Parametric bootstrapping ${\rm Var}_{F_\theta}(\hat{\theta})$
In this section, I generate a dataset of size $n = 50$ from the distribution
$X_i\sim{\rm iid}U(0,3)$ and use it to approximate
${\rm Var}_{F_\theta}(\hat{\theta})$ using $B=5000$ parametric bootstrap
samples. The difference between parametric and nonparametric bootstrap is that
in the parametric bootstrap I generate samples from $U(0,\hat{\theta})$ and in
the nonparametric bootstrap I generate samples from the emperical distribution.
In this section, I perform the parametric bootstrap.

```{r}
## Preamble
set.seed(33)

## Import external libraries
library(ggplot2)
library(latex2exp)

## Parameterize
n = 50
t = 3
B = 5000

## Generate original dataset
xs = runif(n, 0, t)

## Generate parametric bootstrap samples and calculate the sample statistic
## variance

# generate samples
that = max(xs)
bs_param_raw = runif(n*B, 0, that)
bs_param = matrix(bs_param_raw, nrow=B, ncol=n)

# calculate max of each sample
maxs_param = apply(bs_param, 1, max) # 1 means apply to rows, 2 would be columns

# calculate the parametric bootstrapped variance of \hat{\theta}
var_boot_param = var(maxs_param)

## Calculate the analytical variance
var_anal = n*t^2*(1/(n+2) - n/(n+1)^2)

## Plot parametric bootstrap samples
plt_var_param = ggplot(data=data.frame(x=maxs_param), aes(x=x)) +
    geom_histogram(color="steelblue", binwidth=0.05) +
    geom_vline(aes(xintercept=t), size=1.5, color="coral") +
    labs(
        x=TeX("Resampled estimate of $\\hat{\\theta}$"),
        y="Frequency",
        title=
        TeX("Parametric bootstrapping to estimate Var($\\hat{\\theta}$)")
    )
```

```{r echo=F}
plt_var_param
```
  
The parametric bootstrapped variance of $\hat{\theta}$ is $`r var_boot_param`$
and the analytical variance thereof is $`r var_anal`$.
The log-ratio of these two values is $`r log(var_boot_param/var_anal)`$,
suggesting that the parametric bootstrapped estimate is relatively accurate in
this case.

## Nonparametric bootstrapping the same quantity
In this section, I perform the nonparametric bootstrap to estimate the variance
of the MLE $\hat{\theta}={\rm max}(\{X_1,\dots,X_n\})$.

```{r}
## Generate nonparametric bootstrap samples and calculate var(\hat{\theta})

# generate bootstrap
bs_nonparam_raw = sample(xs, size=n*B, replace=TRUE)
bs_nonparam = matrix(data=bs_nonparam_raw, nrow=B, ncol=n)

# calculate max of each bootstrap sample
maxs_nonparam = apply(bs_nonparam, 1, max)

# calculate nonparametric bootstrapped variance of the mle
var_boot_nonparam = var(maxs_nonparam)

## Plot variances
plt_var_nonparam = ggplot(data=data.frame(x=maxs_nonparam), aes(x=x)) +
    geom_histogram(color="steelblue", binwidth=0.05) +
    geom_vline(aes(xintercept=t), size=1.5, color="coral") +
    labs(
        x=TeX("Resampled estimate of $\\hat{\\theta}$"),
        y="Frequency",
        title=
        TeX("Nonparametric bootstrapping to estimate Var($\\hat{\\theta}$)")
    )

### END: Code
```

```{r echo=F}
plt_var_nonparam
```
  
The nonparametric bootstrapped variance of
$\hat{\theta}$ is $`r var_boot_nonparam`$ and the analytical variance thereof is
$`r var_anal`$.  The log-ratio of these two values is
$`r log(var_boot_nonparam/var_anal)`$, suggesting that the bootstrapped estimate
is somewhat accurate in this case.

## Comparison of parametric and nonparametric bootstrapping
In the case where $X_i\sim{\rm iid}U(0,\theta)$, parametric bootstrap estimates
of the the variance of the MLE estimator
$\hat{\theta} \triangleq {\rm max}(\{X_1,\dots,X_n\})$ are more accurate than
their nonparametric counterparts.

$$
\frac{|{\rm Var}_{F_\theta}(\hat{\theta}) - {\rm Var}_{p}(\hat{\theta})|}
{|{\rm Var}_{F_\theta}(\hat{\theta}) - {\rm Var}_{np}(\hat{\theta})|} =
\frac{`r abs(var_anal - var_boot_param)`}
{`r abs(var_anal - var_boot_nonparam)`} \approx
`r abs(var_anal - var_boot_param)/abs(var_anal - var_boot_nonparam)`
$$
That this ratio is less than $1$ provides evidence for the claim that the
parametric bootstrap is more accurate in this case. At a glance, the
nonparametric bootstrap appears to underestimate the actual variance of the MLE
$\hat{\theta}$. One reason for this appears to be that the efficiency of the
nonparametric bootstrap is tied strongly to the sample size than the parametric
bootstrap, but this would require a technical proof to verify.

# Bootstrapping regression coefficients
In this section, I examine bootstrapping as a method to estimate confidence
intervals on regression coefficients.

## Generating observations
I consider two models and generate $n=512$ observations from each. The first
model is described by the following algorithm, additional source code found in
refference $[3]$.

```{r}
## Import external source
source("genetic_supplement.R")

## Parameterize
n = 512

## Generate observations
x1s = (0:(n-1))/n
fx1s = model1(x1s)
y1s = fx1s + noise1(x1s)

## Plot generated observations
plt_mod1 = ggplot(data=data.frame(x=x1s, y=y1s, f=fx1s)) +
    geom_point(aes(x=x, y=y), color="steelblue", alpha=0.8) +
    geom_line(aes(x=x, y=f), color="coral", size=0.75, alpha=0.5) +
    labs(x="X", y="Y", title="Model 1")
```

```{r echo=F}
plt_mod1
```

The second model is
$$f(x) = (4x − 2) + 2 e^{−16(4x − 2)^2}$$

```{r}
## Subroutines
model2 = function(x){
    r = (4*x-2) + 2*exp(-16*(4*x-2)^2)
    return(r)
}

## Generate observations
x2s = (0:(n-1))/n
fx2s = model2(x2s)
y2s = fx2s + rnorm(n, sd=(fx2s^2/5))

## Plot generated observations
plt_mod2 = ggplot(data=data.frame(x=x2s, y=y2s, f=fx2s)) +
    geom_point(aes(x=x, y=y), color="steelblue", alpha=0.8) +
    geom_line(aes(x=x, y=f), color="coral", size=0.75, alpha=0.5) +
    labs(x="X", y="Y", title="Model 2")
```

```{r echo=F}
plt_mod2
```

## Genetic algorithm for calculating regression curve estimates

## Bootstrapped 95% confidence bands for regression curves

# Estimating quantile range with the jackknife
In this section, I use the bootstrap and its predesessor, the jackknife, to
simulate confidence intervals for $\theta$.

$$
\theta \triangleq \frac{q_{0.75}-q_{0.25}}{1.34} \quad {\rm for} \quad
X_1,\dots,X_n \sim {\rm iid} \, t_{df} \quad {\rm using} \quad
n = 25 \, {\rm and} \, df=3
$$

```{r}
q1 = qt(0.75, df=3)
q0 = qt(0.25, df=3)
theta = (q1 - q0) / 1.34
```

The true value of $\theta$ is $\frac{`r q1`-(`r q0`)}{1.34} \approx `r theta`$.

## Generating observations
```{r}
## Parameterize
n = 25
df = 3

## Generate observations
xs = rt(n=n, df=df)

## Plot
plt_ts = ggplot(data=data.frame(x=xs)) +
    geom_histogram(aes(x=x), color="steelblue", bins=30) +
    labs(x="X", y="Frequency", title="Histogram of generated observations")
```

```{r echo=F}
plt_ts
```

# Sources
1. https://en.wikipedia.org/wiki/Order_statistic
2. https://en.wikipedia.org/wiki/Variance
3. https://github.com/ekalosak/STA243/blob/master/hw6/genetic_supplement.R

---
title: "STA243 Hw5"
author:
- Eric Kalosa-Kenyon
date: "`r format(Sys.time(), '%d %B %Y')`"

abstract: In this assignment, we fit a cubic penalized regression spline using four different methods for obtaining a smoothing parameter.

output:
  html_document:
    toc: yes
---

# Simulation setup

For my simulations, I consider the goodness of fit of cubic penalized regression
splines to the 1-dimensional function $f(x)$. A penalized regression spline is a
function of the form

$$
\hat{f}(x) = \sum_{i=1}^p \beta_i x^i +
    \sum_{j=1}^k \beta_{p_j}(x-t_j)^p_{+}
$$

Where $(x)_{+} = \textrm{max}(x, 0)$, $p$ is the degree of the spline, the
$t_j$ are the locations of the knots,
and $\beta = [\beta_0, \dots, \beta_{p_k}]$ are the regression coefficients.

An example $f(x)$ follows, and I will use it repeatedly to contextualize
theoretical statements throughout this report.

$$
f(x) = 1.5*\Phi(\frac{x-0.35}{0.15}) - \Phi(\frac{x-0.8}{0.04})
\quad \textrm{where} \quad
\Phi(u) = \frac{1}{\sqrt{2\pi}} e^{-u^2/2}
$$

```{r}
## Imports
library(ggplot2)
library(stats)
library(reshape)
library(latex2exp)

## Subroutines
f = function(x){
    # prob density function defined in problem specification
    1.5*dnorm((x-0.35)/0.15) - dnorm((x-0.8)/0.04)
}

## Parameterize
n = 200
is = 1:n
xis = (is-0.5)/n

## Plotting
pdf_df = data.frame(x=xis, y=f(xis))
plt_pdf = ggplot(data=pdf_df) +
    geom_line(aes(x=x, y=y), color="steelblue") +
    labs(x="X", y="Y", title="True f(x)")
```

```{r echo=FALSE}
plt_pdf
```

In this report, I simulate 4 different methods for selecting a smoothing
parameter $\lambda$ under 6 different noise, design density, spatial variation,
and variance heterogeneity states.

# Introduction
In this section, I introduce, the concepts required for the simulation in the
context of a simple example: $y=f(x)+\epsilon$. For the rest of this section,
the reader may assume, unless I state otherwise, that $f(x)$ is the example
function defined in the previous section.

In this report, I consider four methods for fitting the smoothing parameter
$\lambda$ for a penalized cubic regression spline. The cubic spline
$\hat{f}_\lambda(x)$ approximates $f(x)$ and is defined as the minimizer of the
following functional within the class $\mathbf{F}$ of cubic regression splines:

$$
\hat{f}_\lambda(x) =
\textrm{argmin}_{f\in\mathbf{F}} \frac{1}{n}\sum_{i=1}^{n}(y_i-f(x_i))^2 +
    \lambda\sum_{j=1}^{k} \beta_{p_j}^2 =
||y-f(x)||_2^2 + \lambda\beta^\top D\beta
$$

where $D$ is the diagonal matrix with $p+1$ $0$s followed by $K$ $1$s.

As David Ruppert, M. P. Wand, and R. J. Carroll (Semiparametric Regression 2003)
show, the solution to minimizing the functional above follows the form

$$
\hat{f}_\lambda(x) = x(x^\top x+\lambda D)^{-1} x^\top y = H_\lambda y
$$

The following `R` code calculates the matrix $H_\lambda$, generates noisy
observations from the previously defined example function $f(x)$, and fits
penalized cubic regression splines for a number of evocative values of
$\lambda$.

```{r}
## Subroutines
noise_sd = function(j){
    # varaible noise schedule
    return(0.02+0.04*(j-1)^2)
}

## Parameterize
J = 6 # for generating noisy observations
k = 30 # number of knots
p = 3 # cubic spline
a = 0
b = 1 # bounds of inegration for lambda*int_a^b f^''(x)^2 dx penalty
knots = seq(a, b, length.out=k)

fxs = f(xis)
noise_df = data.frame(x=xis, yt=fxs)
for(j in 1:J){
    yis = fxs + rnorm(n, mean=0, sd=noise_sd(j))
    colname = paste("y", j, sep="")
    noise_df[colname] = yis
}

# Perform cubic penalized spline regression for multiple lambda values
ex_xs = noise_df$x
ex_ys = noise_df$y3 # arbitrary noise profile for example
lam_df = data.frame(x=xis, y=fxs, yn=ex_ys)
lams = 10^seq(-10, 5, length.out=5)

D = diag(c(rep(0, p+1), rep(1, k)))
X1 = outer(ex_xs, 0:p, "^")
X2 = outer(ex_xs, knots, ">")*outer(ex_xs, knots, "-")^p # source [2]
X = cbind(X1, X2)
Y = ex_ys

for(lam in lams){
    H = X %*% solve(t(X) %*% X + lam*D) %*% t(X)
    fhys = H %*% Y
    colname = paste("lambda", lam, sep="=")
    lam_df[colname] = fhys
}

plt_mx = ggplot(data=lam_df) +
    geom_line(data=melt(lam_df, id=c("x", "y", "yn")),
              aes(x=x, y=value, color=variable)) +
    geom_point(aes(x=x, y=yn), alpha=0.4) +
    labs(x="X", y="Y",
        title=paste(
            "Example cubic penalized regression spline with", k, "knots"
        )
    ) +
    scale_colour_discrete(
        labels=as.character(lams),
        name=TeX("Penalty $\\lambda$")
    )
plt_mx
```

# Smoothing parameter selection
In this report, I am concerned with the selection of the smoothing parameter
$\lambda$.
As stated in the introduction, I consider four methods for selecting the
optimal smoothing parameter $\lambda$. In this section, I describe and perform
cross validation, generalized cross validation, improved AIC, and $L_2$ risk
minimization on a single experimental simulation setting. In the following
section, I perform these methods on a variety of experimental simulation
settings, but I've simplified this section to expose the methodology to the
reader.

## Cross validation (CV)

$$
CV(\lambda) = \frac{1}{N}\sum_{i=1}^{N}
    (\frac{y_i-\hat{f}_\lambda(x_i)}{1-(S_\lambda)_{i,i}})^2
$$

```{r}
cv = function(l, k=k, X=X, Y=Y){
    # k is num knots
    # l is lambda, penalty
    # X is design matrix
    # Y is response

    n = dim(X)[1]
    D = diag(c(rep(0, dim(X)[2]-k), rep(1, k)))
    H = X %*% solve(t(X) %*% X + l*D) %*% t(X)
    fhys = H %*% Y
    hii = diag(H)
    r = mean(((Y-fhys)/(1-hii))^2)
    return(r)
}

cvv = Vectorize(cv, vectorize.args = c("l"))
lambdas = 10^(seq(-10, -4, length.out=100))
cvs = cvv(X=X, Y=Y, l=lambdas, k=k)

cv_df = data.frame(x=lambdas, y=cvs)
plt_cv = ggplot(data=cv_df) +
    geom_line(aes(x=x, y=y), color="steelblue") +
    labs(x="Smoothing Penalty", y="Cross Validation Score",
         title="Cross validated values of lambda") +
    scale_x_log10() +
    # geom_hline(yintercept = min(cvs), color="coral") +
    geom_vline(xintercept = lambdas[which(cvs == min(cvs))], color="coral")
```

```{r echo=F}
plt_cv
```

Note that the minimizing smoothing penalty is
$\lambda_{CV}=`r lambdas[which(cvs == min(cvs))]`$
with a cross validation score of
$CV(\lambda_{CV})=`r min(cvs)`$.

## Generalized cross validation (GCV)

$$
GCV(\lambda) = \frac{\frac{1}{N}\sum_{i=1}^{N}
    (y_i-\hat{f}_\lambda(x_i))^2}{(1-\frac{1}{N}\textrm{tr}(S_\lambda))^2}
$$

```{r}
gcv = function(X=X, Y=Y, k=k, l){
    # k is num knots
    # l is lambda, penalty
    # X is design matrix
    # Y is response

    n = dim(X)[1]
    D = diag(c(rep(0, dim(X)[2]-k), rep(1, k)))
    H = X %*% solve(t(X) %*% X + l*D) %*% t(X)

    fhys = H %*% Y
    tr = diag(H)

    numer = mean((Y-fhys)^2)
    denom = (1-mean(tr))^2
    r = numer/denom
    return(r)
}

gcvv = Vectorize(gcv, vectorize.args = c("l"))
lambdas = 10^(seq(-9, -4, length.out=100))
gcvs = gcvv(X=X, Y=Y, l=lambdas, k=k)

gcv_df = data.frame(x=lambdas, y=gcvs)
plt_gcv = ggplot(data=gcv_df) +
    geom_line(aes(x=x, y=y), color="steelblue") +
    labs(x="Smoothing Penalty", y="Generalized Cross Validation Score",
         title="Generalized cross validated values of lambda") +
    scale_x_log10() +
    geom_vline(xintercept = lambdas[which(gcvs == min(gcvs))], color="coral")
```

```{r echo=F}
plt_gcv
```

Note that the minimizing smoothing penalty is
$\lambda_{GCV}=`r lambdas[which(gcvs == min(gcvs))]`$
with a generalized cross validation score of
$GCV(\lambda_{GCV})=`r min(gcvs)`$.


## Akaike information criterion (AIC)

$$
AIC_C(\lambda) = \textrm{log}(||y-\hat{f}_\lambda||_2^2) +
    \frac{2(\textrm{tr}(H_\lambda)+1)}{n-\textrm{tr}(H_\lambda)-2}
$$

```{r}
aicc = function(X=X, Y=Y, k=k, l){
    # k is num knots
    # l is lambda, penalty
    # X is design matrix
    # Y is response

    n = dim(X)[1]
    D = diag(c(rep(0, dim(X)[2]-k), rep(1, k)))
    H = X %*% solve(t(X) %*% X + l*D) %*% t(X)

    fhys = H %*% Y
    tr = sum(diag(H))

    r1 = log(mean((Y-fhys)^2))
    r2n = 2*(tr+1)
    r2d = n - tr - 2
    r = r1 + r2n/r2d
    return(r)
}

aiccv = Vectorize(aicc, vectorize.args = c("l"))
lambdas = 10^(seq(-8, -3, length.out=100))
aiccs = aiccv(X=X, Y=Y, l=lambdas, k=k)

aicc_df = data.frame(x=lambdas, y=aiccs)
plt_aicc = ggplot(data=aicc_df) +
    geom_line(aes(x=x, y=y), color="steelblue") +
    labs(x="Smoothing Penalty", y="Corrected AIC Score",
         title="Corrected AIC scores for smoothing parameter") +
    scale_x_log10() +
    geom_vline(xintercept = lambdas[which(aiccs == min(aiccs))], color="coral")
```

```{r echo=F}
plt_aicc
```

Note that the minimizing smoothing penalty is
$\lambda_{AIC_C}=`r lambdas[which(aiccs == min(aiccs))]`$
with a generalized cross validation score of
$AIC_C(\lambda_{AIC_C})=`r min(aiccs)`$.

## Emperical risk minimization (ERM)

# Comparison of methods for smoothing parameter selection

## Performance under different levels of noise

## Performance under different design density

## Performance under different frequency of target function

## Performance under heterogeneous variances

# Sources
1. http://data.princeton.edu/eco572/smoothing.pdf
2. http://people.stat.sfu.ca/~cschwarz/Consulting/Trinity/Phase2/TrinityWorkshop/Workshop-handouts/TW-04-Intro-splines.pdf
3. Smoothing parameterselection forsmoothing splines: a simulation study
   Thomas C.M. Lee
   (Computational Statistics & Data Analysis 42 (2003) 139 â€“ 148)
4. http://staff.ustc.edu.cn/~zwp/teach/nonpar/Spline%20and%20penalized%20regression.pdf

---
title: "STA243 Hw5"
author:
- Eric Kalosa-Kenyon
date: "`r format(Sys.time(), '%d %B %Y')`"

abstract: In this assignment, we fit a cubic penalized regression spline using four different methods for obtaining a smoothing parameter.

output:
  html_document:
    toc: yes
---

# Simulation setup

For our simulations, we consider the true 1-dimensional function $f(x)$:
$$
f(x) = 1.5*\Phi(\frac{x-0.35}{0.15}) - \Phi(\frac{x-0.8}{0.04})
\quad \textrm{where} \quad
\Phi(u) = \frac{1}{\sqrt{2\pi}} e^{-u^2/2}
$$

```{r}
## Imports
library(ggplot2)
library(stats)
library(reshape)

## Subroutines
f = function(x){
    # prob density function defined in problem specification
    1.5*dnorm((x-0.35)/0.15) - dnorm((x-0.8)/0.04)
}

## Parameterize
n = 200
is = 1:n
xis = (is-0.5)/n

## Plotting
pdf_df = data.frame(x=xis, y=f(xis))
plt_pdf = ggplot(data=pdf_df) +
    geom_line(aes(x=x, y=y), color="steelblue") +
    labs(x="X", y="Y", title="True f(x)")
```

```{r echo=FALSE}
plt_pdf
```

# Introduction
We consider four methods for fitting the smoothing parameter $\lambda$ for a
penalized cubic regression spline. The cubic spline $\hat{f}_\lambda(x)$
approximates $f(x)$ and is defined as the minimizer of the following functional:

$$
\hat{f}_\lambda(x) =
\textrm{argmin}_{f\in\mathbf{F}} \frac{1}{n}\sum_{i=1}^{n}(y_i-f(x_i))^2 +
    \lambda*\int_{a}^{b} (\frac{\partial^2}{\partial x^2} f(x))^2 dx
$$

 As is shown in chapter 2 of
Green and Silverman (1994), the solution to minimizing the functional above is a
natural cubic penalized regression spline, making calculating the penalty much
simpler:

$$
S(f) = \frac{1}{n}\sum_{i=1}^{n}(y_i-f(x_i))^2 +
    \lambda*\int_{a}^{b} (\frac{\partial^2}{\partial x^2} f(x))^2 dx =
(y-f(x))^\top (y-f(x)) + \lambda*f(x)^\top K f(x)
$$

$$
K = \Delta W^{-1} \Delta
\quad \textrm{where} \quad
    \Delta_{i,i} = 1/h_i,
    \Delta_{i,i+1} = -1/h_i - 1/h_{i+1},
    \Delta_{i,i+2} = 1/h_{i, i+1},
    W_{i-1,i} = W_{i,i-1} h_i/6,
    \quad \textrm{and} \quad
    W_{i,i} = (h_i+h_{i+1})/3
$$
and $h_i$ is the distance between the $i$ and $i+1$ knots. The following `R`
code calculates the matrix $K$ from the equtions above:

```{r}
## Subroutines
noise_sd = function(j){
    return(0.02+0.04*(j-1)^2)
}

## Parameterize
J = 6 # for generating noisy observations
k = 30 # number of knots
a = 0
b = 1 # bounds of inegration for lambda*int_a^b f^''(x)^2 dx penalty
knots = seq(a, b, length.out=k)

fxs = f(xis)
noise_df = data.frame(x=xis, yt=fxs)
for(j in 1:J){
    yis = fxs + rnorm(n, mean=0, sd=noise_sd(j))
    colname = paste("y", j, sep="")
    noise_df[colname] = yis
}

h = knots[2] - knots[1]
D = matrix(0, nrow = n-2, ncol = n)
W = matrix(0, nrow = n-2, ncol = n-2)

for(i in 1:(n-2)){
    D[i,i] = 1/h
    D[i, i+1] = -1/h-1/h
    D[i, i+2] = 1/h
    if(i>1){
        W[i-1, i] = h/6
        W[i, i-1] = h/6
    }
    W[i, i] = 2*h/3
}
K = t(D)%*%solve(W)%*%D

# Perform cubic penalized spline regression for multiple lambda values
ex_ys = noise_df$y3 # arbitrary noise profile for example
lam_df = data.frame(x=xis, y=fxs, yn=ex_ys)
lams = 10^(-2:2)

for(lam in lams){
    fit = solve(diag(n) + lam*K)%*%ex_ys
    colname = paste("lambda", lam, sep="=")
    lam_df[colname] = fit
}

plt_mx = ggplot(data=lam_df) +
    geom_line(data=melt(lam_df, id=c("x", "y", "yn")),
              aes(x=x, y=value, color=variable)) +
    geom_point(aes(x=x, y=yn), alpha=0.4) +
    labs(x="X", y="Y",
        title=paste(
            "Example cubic penalized regression spline with", k, "knots"
        )
    )
plt_mx
```

# Smoothing parameter selection
Here, we are concerned with the selection of the smoothing parameter $\lambda$.
As stated in the introduction, we consider four methods for selecting the
optimal smoothing parameter $\lambda$. In this section, the four proceedures are
described and executed.

## Cross validation (CV)

$$
CV(\lambda) = \frac{1}{N}\sum_{i=1}^{N}
    (\frac{y_i-\hat{f}_\lambda(x_i)}{1-(S_\lambda)_{i,i}})^2
$$

```{r}
cv = function(l, nknot=k, K=K, xs=xis, ys){
    # fh = smooth.spline(df=3, x=xs, y=ys, nknots=nknot, lambda=l)
    # fhys = predict(fh, xs)[[2]]
    n = length(xis)
    S = solve(diag(n) + l*K)
    fhys = S %*% ys
    sii = diag(S)
    r = mean(((ys-fhys)/(1-sii))^2)
    return(r)
}

cvv = Vectorize(cv, vectorize.args = c("l"))
lambdas = 10^(seq(-6, 0.5, length.out=100))
cvs = cvv(xs=xis, ys=ex_ys, l=lambdas, nknot=k, K=K)

cv_df = data.frame(x=lambdas, y=cvs)
plt_cv = ggplot(data=cv_df) +
    geom_line(aes(x=x, y=y), color="steelblue") +
    labs(x="Smoothing Penalty", y="Cross Validation Score",
         title="Cross validating values of lambda") +
    scale_x_log10() +
    # geom_hline(yintercept = min(cvs), color="coral") +
    geom_vline(xintercept = lambdas[which(cvs == min(cvs))], color="coral")
```

```{r echo=F}
plt_cv
```

Note that the minimizing lambda is
$\lambda_{CV}=`r lambdas[which(cvs == min(cvs))]`$
with a cross validation score of
$CV(\lambda_{CV})=`r min(cvs)`$.
Using the built in penalized regression spline `R` function, we find

```{r}
builtin_cv = smooth.spline(x=xis, y=ex_ys, nknots=k, cv=TRUE)
```

a cross validated $\lambda_{CV}^\star=`r builtin_cv$lambda`$. Notice that
$\lambda_{CV}$ and $\lambda_{CV}^\star$ may be different due to different
implementations of cross validation proceedures, the imprecise, unsettled
terminology in the spline literature, and the stochastic noise generation
proceedures.

## Generalized cross validation (GCV)

## Akaike information criterion (AIC)

## Emperical risk minimization (ERM)

# Comparison of methods for smoothing parameter selection

## Performance under different levels of noise

## Performance under different design density

## Performance under different frequency of target function

## Performance under heterogeneous variances

# Sources
1. http://data.princeton.edu/eco572/smoothing.pdf
2. http://people.stat.sfu.ca/~cschwarz/Consulting/Trinity/Phase2/TrinityWorkshop/Workshop-handouts/TW-04-Intro-splines.pdf
3. Smoothing parameterselection forsmoothing splines: a simulation study
   Thomas C.M. Lee
   (Computational Statistics & Data Analysis 42 (2003) 139 â€“ 148)
4. http://staff.ustc.edu.cn/~zwp/teach/nonpar/Spline%20and%20penalized%20regression.pdf

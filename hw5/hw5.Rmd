---
title: "STA243 Hw5"
author:
- Eric Kalosa-Kenyon
date: "`r format(Sys.time(), '%d %B %Y')`"

abstract: In this assignment, we fit a cubic penalized regression spline using four different methods for obtaining a smoothing parameter.

output:
  html_document:
    toc: yes
---

# Simulation setup

For my simulations, I consider the goodness of fit of cubic penalized regression
splines to the 1-dimensional function $f(x)$. A penalized regression spline is a
function of the form

$$
\hat{f}(x) = \sum_{i=1}^p \beta_i x^i +
    \sum_{j=1}^k \beta_{p_j}(x-t_j)^p_{+}
$$

Where $(x)_{+} = \textrm{max}(x, 0)$, $p$ is the degree of the spline, the
$t_j$ are the locations of the knots,
and $\beta = [\beta_0, \dots, \beta_{p_k}]$ are the regression coefficients.

An example $f(x)$ follows, and I will use it repeatedly to contextualize
theoretical statements throughout this report.

$$
f(x) = 1.5*\Phi(\frac{x-0.35}{0.15}) - \Phi(\frac{x-0.8}{0.04})
\quad \textrm{where} \quad
\Phi(u) = \frac{1}{\sqrt{2\pi}} e^{-u^2/2}
$$

```{r}
## Imports
library(ggplot2)
library(stats)
library(reshape)
library(latex2exp)

## Subroutines
f = function(x){
    # prob density function defined in problem specification
    1.5*dnorm((x-0.35)/0.15) - dnorm((x-0.8)/0.04)
}

## Parameterize
n = 200
is = 1:n
xis = (is-0.5)/n

## Plotting
pdf_df = data.frame(x=xis, y=f(xis))
plt_pdf = ggplot(data=pdf_df) +
    geom_line(aes(x=x, y=y), color="steelblue") +
    labs(x="X", y="Y", title="True f(x)")
```

```{r echo=FALSE}
plt_pdf
```

In this report, I simulate 4 different methods for selecting a smoothing
parameter $\lambda$ under 6 different noise, design density, spatial variation,
and variance heterogeneity states.

# Introduction
In this section, I introduce, the concepts required for the simulation in the
context of a simple example: $y=f(x)+\epsilon$. For the rest of this section,
the reader may assume, unless I state otherwise, that $f(x)$ is the example
function defined in the previous section.

In this report, I consider four methods for fitting the smoothing parameter
$\lambda$ for a penalized cubic regression spline. The cubic spline
$\hat{f}_\lambda(x)$ approximates $f(x)$ and is defined as the minimizer of the
following functional within the class $\mathbf{F}$ of cubic regression splines:

$$
\hat{f}_\lambda(x) =
\textrm{argmin}_{f\in\mathbf{F}} \sum_{i=1}^{n}(y_i-f(x_i))^2 +
    \lambda\sum_{j=1}^{k} \beta_{p_j}^2 =
||y-f(x)||_2^2 + \lambda\beta^\top D\beta
$$

where $D$ is the diagonal matrix with $p+1$ $0$s followed by $K$ $1$s.

As David Ruppert, M. P. Wand, and R. J. Carroll (Semiparametric Regression 2003)
show, the solution to minimizing the functional above follows the form

$$
\hat{f}_\lambda(x) = x(x^\top x+\lambda D)^{-1} x^\top y = H_\lambda y
$$

The following `R` code calculates the matrix $H_\lambda$, generates noisy
observations from the previously defined example function $f(x)$, and fits
penalized cubic regression splines for a number of evocative values of
$\lambda$.

```{r}
## Subroutines
noise_sd = function(j){
    # varaible noise schedule
    return(0.02+0.04*(j-1)^2)
}

## Parameterize
J = 6 # for generating noisy observations
k = 30 # number of knots
p = 3 # cubic spline
a = 0
b = 1 # bounds of inegration for lambda*int_a^b f^''(x)^2 dx penalty
knots = seq(a, b, length.out=k)

fxs = f(xis)
noise_df = data.frame(x=xis, yt=fxs)
for(j in 1:J){
    yis = fxs + rnorm(n, mean=0, sd=noise_sd(j))
    colname = paste("y", j, sep="")
    noise_df[colname] = yis
}

# Perform cubic penalized spline regression for multiple lambda values
ex_xs = noise_df$x
ex_ys = noise_df$y3 # arbitrary noise profile for example
lam_df = data.frame(x=xis, y=fxs, yn=ex_ys)
lams = 10^seq(-10, 5, length.out=5)

D = diag(c(rep(0, p+1), rep(1, k)))
X1 = outer(ex_xs, 0:p, "^")
X2 = outer(ex_xs, knots, ">")*outer(ex_xs, knots, "-")^p # source [2]
X = cbind(X1, X2)
Y = ex_ys

for(lam in lams){
    H = X %*% solve(t(X) %*% X + lam*D) %*% t(X)
    fhys = H %*% Y
    colname = paste("lambda", lam, sep="=")
    lam_df[colname] = fhys
}

plt_mx = ggplot(data=lam_df) +
    geom_line(data=melt(lam_df, id=c("x", "y", "yn")),
              aes(x=x, y=value, color=variable)) +
    geom_point(aes(x=x, y=yn), alpha=0.4, shape=18, size=1.5) +
    labs(x="X", y="Y",
        title=paste(
            "Example cubic penalized regression spline with", k, "knots"
        )
    ) +
    scale_colour_discrete(
        labels=as.character(lams),
        name=TeX("Penalty $\\lambda$")
    )
plt_mx
```

# Smoothing parameter selection
In this report, I am concerned with the selection of the smoothing parameter
$\lambda$.
As stated in the introduction, I consider four methods for selecting the
optimal smoothing parameter $\lambda$. In this section, I describe and perform
cross validation, generalized cross validation, improved AIC, and $L_2$ risk
minimization on a single experimental simulation setting. In the following
section, I perform these methods on a variety of experimental simulation
settings, but I've simplified this section to expose the methodology to the
reader.

## Cross validation (CV)

$$
CV(\lambda) = \frac{1}{N}\sum_{i=1}^{N}
    (\frac{y_i-\hat{f}_\lambda(x_i)}{1-(S_\lambda)_{i,i}})^2
$$

```{r}
cv = function(l, k=k, X=X, Y=Y){
    # k is num knots
    # l is lambda, penalty
    # X is design matrix
    # Y is response

    n = dim(X)[1]
    D = diag(c(rep(0, dim(X)[2]-k), rep(1, k)))
    H = X %*% solve(t(X) %*% X + l*D) %*% t(X)
    fhys = H %*% Y
    hii = diag(H)
    r = mean(((Y-fhys)/(1-hii))^2)
    return(r)
}

cvv = Vectorize(cv, vectorize.args = c("l"))
lambdas = 10^(seq(-10, -4, length.out=100))
cvs = cvv(X=X, Y=Y, l=lambdas, k=k)

cv_df = data.frame(x=lambdas, y=cvs)
plt_cv = ggplot(data=cv_df) +
    geom_line(aes(x=x, y=y), color="steelblue") +
    labs(x="Smoothing Penalty", y="Cross Validation Score",
         title="Cross validated values of lambda") +
    scale_x_log10() +
    # geom_hline(yintercept = min(cvs), color="coral") +
    geom_vline(xintercept = lambdas[which(cvs == min(cvs))], color="coral")
```

```{r echo=F}
lcv = lambdas[which(cvs == min(cvs))]
plt_cv
```

Note that the minimizing smoothing penalty is
$\lambda_{CV}=`r lcv`$
with a cross validation score of
$CV(\lambda_{CV})=`r min(cvs)`$.

## Generalized cross validation (GCV)

$$
GCV(\lambda) = \frac{\frac{1}{N}\sum_{i=1}^{N}
    (y_i-\hat{f}_\lambda(x_i))^2}{(1-\frac{1}{N}\textrm{tr}(S_\lambda))^2}
$$

```{r}
gcv = function(X=X, Y=Y, k=k, l){
    # k is num knots
    # l is lambda, penalty
    # X is design matrix
    # Y is response

    n = dim(X)[1]
    D = diag(c(rep(0, dim(X)[2]-k), rep(1, k)))
    H = X %*% solve(t(X) %*% X + l*D) %*% t(X)

    fhys = H %*% Y
    tr = diag(H)

    numer = mean((Y-fhys)^2)
    denom = (1-mean(tr))^2
    r = numer/denom
    return(r)
}

gcvv = Vectorize(gcv, vectorize.args = c("l"))
lambdas = 10^(seq(-9, -4, length.out=100))
gcvs = gcvv(X=X, Y=Y, l=lambdas, k=k)

gcv_df = data.frame(x=lambdas, y=gcvs)
plt_gcv = ggplot(data=gcv_df) +
    geom_line(aes(x=x, y=y), color="steelblue") +
    labs(x="Smoothing Penalty", y="Generalized Cross Validation Score",
         title="Generalized cross validated values of lambda") +
    scale_x_log10() +
    geom_vline(xintercept = lambdas[which(gcvs == min(gcvs))], color="coral")
```

```{r echo=F}
plt_gcv
```

Note that the minimizing smoothing penalty is
$\lambda_{GCV}=`r lambdas[which(gcvs == min(gcvs))]`$
with a generalized cross validation score of
$GCV(\lambda_{GCV})=`r min(gcvs)`$.

## Akaike information criterion (AIC)

$$
AIC_C(\lambda) = \textrm{log}(||y-\hat{f}_\lambda||_2^2) +
    \frac{2(\textrm{tr}(H_\lambda)+1)}{n-\textrm{tr}(H_\lambda)-2}
$$

```{r}
aicc = function(X=X, Y=Y, k=k, l){
    # k is num knots
    # l is lambda, penalty
    # X is design matrix
    # Y is response

    n = dim(X)[1]
    D = diag(c(rep(0, dim(X)[2]-k), rep(1, k)))
    H = X %*% solve(t(X) %*% X + l*D) %*% t(X)

    fhys = H %*% Y
    tr = sum(diag(H))

    r1 = log(mean((Y-fhys)^2))
    r2n = 2*(tr+1)
    r2d = n - tr - 2
    r = r1 + r2n/r2d
    return(r)
}

aiccv = Vectorize(aicc, vectorize.args = c("l"))
lambdas = 10^(seq(-8, -3, length.out=100))
aiccs = aiccv(X=X, Y=Y, l=lambdas, k=k)

aicc_df = data.frame(x=lambdas, y=aiccs)
plt_aicc = ggplot(data=aicc_df) +
    geom_line(aes(x=x, y=y), color="steelblue") +
    labs(x="Smoothing Penalty", y="Corrected AIC Score",
         title="Corrected AIC scores for smoothing parameter") +
    scale_x_log10() +
    geom_vline(xintercept = lambdas[which(aiccs == min(aiccs))], color="coral")
```

```{r echo=F}
plt_aicc
```

Note that the minimizing smoothing penalty is
$\lambda_{AIC_C}=`r lambdas[which(aiccs == min(aiccs))]`$
with a score of
$AIC_C(\lambda_{AIC_C})=`r min(aiccs)`$.

## Emperical risk minimization (ERM)

In trying to minimize $risk(\lambda) = E(||f-\hat{f}_\lambda||^2)$, I actually
minimize an estimate of this quantity because the true $f$ is inaccessable.
First, I prove that
$E(||y-\hat{f}_\lambda||^2) = ||(I-H_\lambda)f||^2 +
    \sigma^2(tr(H_\lambda H_\lambda^\top) -2tr(H_\lambda) + n)$
to get a sense for what this estimate will look like.

1. $E(||y-\hat{f}_\lambda||^2) = E(||(I-H_\lambda)y||^2)$ because
$\hat{f}_\lambda = H_\lambda y$ and real matricies have both left and right
distributivity.

2. $E(||(I-H_\lambda)y||^2) = E(||(I-H_\lambda)(f + e)||^2)$ because $y=f+e$.

3. $E(||(I-H_\lambda)(f + e)||^2) =
E(||(I-H_\lambda)f||^2 + ||(I-H_\lambda)e)||^2)$ because $E(e)=0$ by
assumpion and the distributivity property noted in (1).

4. $E(||(I-H_\lambda)f||^2 + ||(I-H_\lambda)e||^2) =
||(I-H_\lambda)f||^2 + E(||(I-H_\lambda)e)||^2$ because the lefthand quantity is
a constant.
It remains to show that $E(||(I-H_\lambda)e||^2) =
\sigma^2(tr(H_\lambda H_\lambda^\top) -2tr(H_\lambda) + n)$

5. $E(||(I-H_\lambda)e||^2) =
\sigma^2tr(I (H_\lambda-I)(H_\lambda-I)^\top I^\top)$ because
$Cov(e) = I\sigma^2$.

6. $tr(I (H_\lambda-I)(H_\lambda-I)^\top I^\top) = \sum_{i=1}^n (h_{ii}-1)^2$.

7. $\sum_{i=1}^n (h_{\lambda,ii} - 1)^2 =
   tr(H_\lambda H_\lambda^\top) -2tr(H_\lambda) + n$

completing the proof.

In the following steps, I develop an estimator for
$risk(\lambda) = E(||f-\hat{f}_\lambda||^2)$:

1. $risk(\lambda) = E(||y-e-\hat{f}_\lambda||^2)$ by the model assumption
   $y=f+e$.
2. Expanding the $L_2$ norm in the expression above, using the linearity of
   expectation, and noting the fact that
   $E(e)=0$;
   $E(||y-e-\hat{f}_\lambda|||^2) =
    E(y^\top y - y^\top \hat{f}_\lambda + e^\top e - \hat{f}_\lambda^\top y +
    \hat{f}_\lambda^\top \hat{f}_\lambda)$
3. Consolidating terms in (2), $risk(\lambda) =
   n\sigma^2 + E(||y-\hat{f}_\lambda||^2)$ and we use the method of moments to
   create an unbiased estimator when $\sigma^2$ is known.

$y$ are the noisy observations, $\hat{f}_\lambda = H_\lambda y$ where
$H_\lambda$ depends only on $X$, and we can create an estimator for $\sigma^2$:
$\hat{\sigma^2} \approx RSS(\hat{f}_\lambda) =
\frac{1}{n} ||y-\hat{f}_\lambda||^2$. Using this estimator for $\sigma^2$,
the estimator for $E(||f-\hat{f}_\lambda||^2)$ is proportional to
$||y-\hat{f}_\lambda||^2$.

An alternative estimator for the risk, using classical pilots, is given in
section 2.2.1 of Lee (2003):
use $\hat{f}_{\lambda_p} \approx f$ where $\lambda_p$ is the $CV$ parameter.
This is the method I employ in my algorithm.

The following algorithm exhibits this risk estimator minimization criterion for
calculating the penalty parameter $\lambda$ for a single experimental simulation
parameterization.

```{r}
sighat = function(Y=Y, X=X, k=k, l){
    D = diag(c(rep(0, dim(X)[2]-k), rep(1, k)))
    H = X %*% solve(t(X) %*% X + l*D) %*% t(X)
    fhys = H %*% Y
    r = mean((Y-fhys)^2)
    return(r)
}

er = function(X=X, Y=Y, k=k, l, lp){
    # k is num knots
    # l is lambda, penalty
    # X is design matrix
    # Y is response

    n = dim(X)[1]
    D = diag(c(rep(0, dim(X)[2]-k), rep(1, k)))
    H = X %*% solve(t(X) %*% X + l*D) %*% t(X)
    Hcv = X %*% solve(t(X) %*% X + lp*D) %*% t(X)

    fcvhys = Hcv %*% Y
    fhys = H %*% Y
    sig = sighat(X=X, Y=Y, k=k, l=lp)

    I = diag(dim(H)[1])
    r = (1/n)*(sum(((H-I)%*%fcvhys)^2) + sig*sum(diag(H%*%t(H))))
    return(r)
}

erv = Vectorize(er, vectorize.args = c("l"))
lambdas = 10^(seq(-10, -3, length.out=100))
ers = erv(X=X, Y=Y, l=lambdas, k=k, lp=lcv)

er_df = data.frame(x=lambdas, y=ers)
plt_er = ggplot(data=er_df) +
    geom_line(aes(x=x, y=y), color="steelblue") +
    labs(x="Smoothing Penalty", y="Estimated Risk",
         title="Expected risk for smoothing parameter") +
    scale_x_log10() +
    geom_vline(xintercept = lambdas[which(ers == min(ers))], color="coral")
```

```{r echo=F}
plt_er
```

Note that the minimizing smoothing penalty is
$\lambda_{ER}=`r lambdas[which(ers == min(ers))]`$
with a score of
$ER(\lambda_{ER})=`r min(ers)`$.

# Comparison of methods for smoothing parameter selection

Now that I've shown the basic behavior of the four penalty parameter estimation
methods, I will examine their performance in a number of simulation settings.

## Performance under different levels of noise
Here I evaluate the performance of $\lambda$ estimators under the model
$y_{ij} = f(x_i) + \sigma_j\epsilon_i$ where $\sigma_j= 0.02 + 0.04(j − 1)^2$
and $\epsilon_i \sim\textrm{iid}N(0,1)$.

```{r}
## Parameterize
J = 2 # number of noise regimes
M = 3 # number of simulations
L = 50 # number of lambdas to grid search
p = 3 # dimension of spline
k = 30 # number of knots
n = 200 # number of observations
a = 0
b = 1
knots = seq(a, b, length.out=k)
xs = ((1:n)-0.5)/n
fxs = f(xs)
lambdas = 10^seq(-10,-5, length.out=L)

## Setup
# Data storage objects
noise_raw_df = data.frame(x=xs, yt=fxs) # observations for plotting
noise_lam_df = data.frame( # best lambda for each simulation 1:M
    cv=rep(NA, J*M), gcv=rep(NA, J*M), aic=rep(NA, J*M), er=rep(NA, J*M),
    J=rep(NA, J*M))
noise_sco_df = data.frame( # score of the corresponding best lambda
    cv=rep(NA, J*M), gcv=rep(NA, J*M), aic=rep(NA, J*M), er=rep(NA, J*M),
    J=rep(NA, J*M))

D = diag(c(rep(0, p+1), rep(1, k)))
X1 = outer(xs, 0:p, "^")
X2 = outer(xs, knots, ">")*outer(xs, knots, "-")^p # source [2]
X = cbind(X1, X2)

for(j in 1:J){

    for(i in 1:M){
        # generate observatons
        Y = fxs + rnorm(n, mean=0, sd=noise_sd(j))

        # calculate lambda using each method
        cvs = cvv(k=k, l=lambdas, X=X, Y=Y)
        gcvs = gcvv(k=k, l=lambdas, X=X, Y=Y)
        aiccs = aiccv(k=k, l=lambdas, X=X, Y=Y)

        bcv = min(cvs) # best cv score is lowest
        cv_lam = lambdas[which(cvs == bcv)]

        ers = erv(k=k, l=lambdas, X=X, Y=Y, lp=cv_lam) # need cv_lam for pilot

        bgcv = min(gcvs)
        gcv_lam = lambdas[which(gcvs == bgcv)]
        baicc = min(aiccs)
        aicc_lam = lambdas[which(aiccs == baicc)]
        ber = min(ers)
        er_lam = lambdas[which(ers == ber)]

        ix = (j-1)*M + i
        noise_lam_df$J[ix] = j
        noise_lam_df$cv[ix] = cv_lam
        noise_lam_df$gcv[ix] = gcv_lam
        noise_lam_df$aic[ix] = aicc_lam
        noise_lam_df$er[ix] = er_lam

        noise_sco_df$cv[ix] = bcv
        noise_sco_df$gcv[ix] = bgcv
        noise_sco_df$aic[ix] = baicc
        noise_sco_df$er[ix] = ber

    }

    # record an example of the observations generated in this noise regime
    colname = paste("J=", j, sep="")
    noise_raw_df[colname] = Y
}

## Plot results
plt_noise_obv = ggplot(data=melt(noise_raw_df, id=c("x", "yt"))) +
    geom_line(data=noise_raw_df, aes(x=x, y=yt), color="steelblue") +
    geom_point(aes(x=x, y=value, color=variable),
               alpha=0.7, size=1.5, shape=18) +
    facet_wrap(~variable) +
    labs(x="X", y="Y", title="Observations with different noise")

noise_lam_df_m = melt(noise_lam_df, id=c("J"))
noise_lam_df_m$J = as.factor(noise_lam_df_m$J)

ylims = boxplot.stats(noise_lam_df_m$value)$stats[c(1, 5)]

plt_noise_lams = ggplot(data=noise_lam_df_m) +
    geom_boxplot(aes(x=J, y=value, color=J), outlier.shape=NA) +
    coord_cartesian(ylim = ylims*1.05) +
    facet_wrap(~variable) +
    scale_y_log10() +
    labs(x="Noise regime", y=TeX("Optimal $\\lambda$"),
         title=TeX("Stability of different methods for estimating $\\lambda$"))
```

```{r echo=F}
plt_noise_obv
```
  
The above plot illustrates the effects of noise regimes $j\in\{1,\dots,J=6\}$.

```{r echo=F}
plt_noise_lams
```
  
As the level of noise increases, the optimal penalty parameter increases and its
stability decreases. The increase in $\lambda$ means that I encourage the spline
to be smoother - to fit less tightly to the noisy data - as the level of noise
increases.

## Performance under different design density
In this section, I examine the effect of different densities of $x_i$ values on
the four different penalty parameter selection methods. I use the following
model as in Lee (2003):

$$
y_{ij} = f(X_{ij}) + \sigma\epsilon_i \quad \textrm{where} \quad
\sigma = 0.1, X_{ij} = F^{-1}(X_i),
F(X) = \textrm{Beta}(\frac{j+4}{5}, \frac{11-j}{5}), X_i\sim \textrm{Unif}(0,1)
$$


```{r}
## Parameterize
J = 2 # number of noise regimes
M = 3 # number of simulations
L = 50 # number of lambdas to grid search
p = 3 # dimension of spline
k = 30 # number of knots
n = 200 # number of observations
a = 0
b = 1
knots = seq(a, b, length.out=k)
lambdas = 10^seq(-9,-5, length.out=L)

## Setup
# Data storage objects
xts = ((1:n)-0.05)/n
yts = f(xts)
dens_raw_df = data.frame(x=xts, y=yts, j=rep(0, n)) # observations for plotting
dens_lam_df = data.frame( # best lambda for each simulation 1:M
    cv=rep(NA, J*M), gcv=rep(NA, J*M), aic=rep(NA, J*M), er=rep(NA, J*M),
    J=rep(NA, J*M))
dens_sco_df = data.frame( # score of the corresponding best lambda
    cv=rep(NA, J*M), gcv=rep(NA, J*M), aic=rep(NA, J*M), er=rep(NA, J*M),
    J=rep(NA, J*M))

D = diag(c(rep(0, p+1), rep(1, k)))

for(j in 1:J){

    for(i in 1:M){

        # variable density Xs
        xs = qbeta(runif(n), (j+4)/5, (11-j)/5)
        fxs = f(xs)
        X1 = outer(xs, 0:p, "^")
        X2 = outer(xs, knots, ">")*outer(xs, knots, "-")^p # source [2]
        X = cbind(X1, X2)

        # generate observatons
        Y = fxs + rnorm(n, mean=0, sd=0.1)

        # calculate lambda using each method
        cvs = cvv(k=k, l=lambdas, X=X, Y=Y)
        gcvs = gcvv(k=k, l=lambdas, X=X, Y=Y)
        aiccs = aiccv(k=k, l=lambdas, X=X, Y=Y)

        bcv = min(cvs) # best cv score is lowest
        cv_lam = lambdas[which(cvs == bcv)]

        ers = erv(k=k, l=lambdas, X=X, Y=Y, lp=cv_lam) # need cv_lam for pilot

        bgcv = min(gcvs)
        gcv_lam = lambdas[which(gcvs == bgcv)]
        baicc = min(aiccs)
        aicc_lam = lambdas[which(aiccs == baicc)]
        ber = min(ers)
        er_lam = lambdas[which(ers == ber)]

        ix = (j-1)*M + i
        dens_lam_df$J[ix] = j
        dens_lam_df$cv[ix] = cv_lam
        dens_lam_df$gcv[ix] = gcv_lam
        dens_lam_df$aic[ix] = aicc_lam
        dens_lam_df$er[ix] = er_lam

        dens_sco_df$cv[ix] = bcv
        dens_sco_df$gcv[ix] = bgcv
        dens_sco_df$aic[ix] = baicc
        dens_sco_df$er[ix] = ber

    }

    # record an example of the observations generated in this noise regime
    tdf = data.frame(x=xs, y=Y, j=rep(j, n)) # observations for plotting
    dens_raw_df = rbind(dens_raw_df, tdf)
}

dens_raw_df$j = as.factor(dens_raw_df$j)
drdf0 = subset(dens_raw_df, j == 0, select=c("x","y"))
plt_dens_obv = ggplot(data=drdf0) +
    geom_line(aes(x=x,y=y), color="steelblue") +
    geom_point(data=subset(dens_raw_df, j != 0), aes(x=x, y=y, color=j),
               shape=18, size=1.5) +
    facet_wrap(~j)

dens_lam_df_m = melt(dens_lam_df, id=c("J"))
dens_lam_df_m$J = as.factor(dens_lam_df_m$J)
ylims = boxplot.stats(dens_lam_df_m$value)$stats[c(1, 5)]
plt_dens_lams = ggplot(data=dens_lam_df_m) +
    geom_boxplot(aes(x=J, y=value, color=J), outlier.shape=NA) +
    scale_y_log10() +
    coord_cartesian(ylim = ylims*1.05) +
    facet_wrap(~variable)
```

```{r echo=F}
plt_dens_obv
```
  
As illustrated above, the small values of $j$ group points to the lefthand side
of the support, while the large values of $j$ group points to the righthand
side.

```{r echo=F}
plt_dens_lams
```
  
The stability of the penalty parameter estimations is much lower when the points
are not homogenously dense thoughout the support. This is especially true
considering I'm fitting a spline with `r k` equally spaced knots. A variable
knot seletion protocol might mitigate some of the instability of the penalty
parameter estimates observed for $j\in\{1,2,5,6\}$.

## Performance under different frequency of target function
In this section I examine the effects of spatial varaition on the stability of
the penalty parameter $\lambda$. I use the following model:

$$
y_{ij} = f_j(x_{i}) + \sigma\epsilon_i \quad \textrm{where} \quad
\sigma = 0.2,
f_j(x) = \sqrt{x*(1-x)}\textrm{sin}(\frac{2\pi(1+2^{(9-4j)/5})}{
x+2^{(9-4j)/5}}), \epsilon_i\sim \textrm{iid}N(0,1)
$$

```{r}
## Parameterize
J = 2 # number of noise regimes
M = 3 # number of simulations
L = 50 # number of lambdas to test
p = 3 # dimension of spline
k = 30 # number of knots
n = 200 # number of observations
a = 0
b = 1
knots = seq(a, b, length.out=k)
xs = ((1:n)-0.5)/n
lambdas = 10^seq(-11,-1, length.out=L)

## Subroutines
ff = function(x, j){
    r = sqrt(x*(1-x))*sin((2*pi*(1+2^((9-4*j)/5)))/(x+2^((9-4*j)/5)))
    return(r)
}

## Setup
# Data storage objects
sp_raw_df = data.frame(x=NA, yt=NA, ys=NA, j=NA) # observations for plotting
sp_lam_df = data.frame( # best lambda for each simulation 1:M
    cv=rep(NA, J*M), gcv=rep(NA, J*M), aic=rep(NA, J*M), er=rep(NA, J*M),
    J=rep(NA, J*M))

D = diag(c(rep(0, p+1), rep(1, k)))
X1 = outer(xs, 0:p, "^")
X2 = outer(xs, knots, ">")*outer(xs, knots, "-")^p # source [2]
X = cbind(X1, X2)

for(j in 1:J){

    fxs = ff(xs, j)

    for(i in 1:M){

        # generate observatons
        Y = fxs + rnorm(n, mean=0, sd=0.2)

        # calculate lambda using each method
        cvs = cvv(k=k, l=lambdas, X=X, Y=Y)
        gcvs = gcvv(k=k, l=lambdas, X=X, Y=Y)
        aiccs = aiccv(k=k, l=lambdas, X=X, Y=Y)

        bcv = min(cvs) # best cv score is lowest
        cv_lam = lambdas[which(cvs == bcv)]

        ers = erv(k=k, l=lambdas, X=X, Y=Y, lp=cv_lam) # need cv_lam for pilot

        bgcv = min(gcvs)
        gcv_lam = lambdas[which(gcvs == bgcv)]
        baicc = min(aiccs)
        aicc_lam = lambdas[which(aiccs == baicc)]
        ber = min(ers)
        er_lam = lambdas[which(ers == ber)]

        ix = (j-1)*M + i
        sp_lam_df$J[ix] = j
        sp_lam_df$cv[ix] = cv_lam
        sp_lam_df$gcv[ix] = gcv_lam
        sp_lam_df$aic[ix] = aicc_lam
        sp_lam_df$er[ix] = er_lam

    }

    # record an example of the observations generated in this noise regime
    tdf = data.frame(x=xs, yt=fxs, ys=Y, j=rep(j, n)) # observations for plotting
    sp_raw_df = rbind(sp_raw_df, tdf)
}

sp_raw_df = subset(sp_raw_df, ! is.na(j)) # remove the NA row
sp_raw_df$j = as.factor(sp_raw_df$j)
plt_sp_obv = ggplot(data=sp_raw_df) +
    geom_line(aes(x=x,y=yt), color="steelblue") +
    geom_point(aes(x=x, y=ys, color=j)) +
    facet_wrap(~j)

sp_lam_df_m = melt(sp_lam_df, id=c("J"))
sp_lam_df_m$J = as.factor(sp_lam_df_m$J)
plt_sp_lams = ggplot(data=sp_lam_df_m) +
    geom_boxplot(aes(x=J, y=value, color=J)) +
    scale_y_log10() +
    facet_wrap(~variable)
```

```{r echo=F}
plt_sp_obv
```
  
As illustrated above, increasing $j$ increases the frequency of the spatial
distribution function $f(x)$.

```{r echo=F}
plt_sp_lams
```
  
Interestingly, increased spatial frequency stabilizes the estimates of the
penalty parameter to a point.

## Performance under heterogeneous variances

# Sources
1. http://data.princeton.edu/eco572/smoothing.pdf
2. http://people.stat.sfu.ca/~cschwarz/Consulting/Trinity/Phase2/TrinityWorkshop/Workshop-handouts/TW-04-Intro-splines.pdf
3. Smoothing parameterselection forsmoothing splines: a simulation study
   Thomas C.M. Lee
   (Computational Statistics & Data Analysis 42 (2003) 139 – 148)
4. http://staff.ustc.edu.cn/~zwp/teach/nonpar/Spline%20and%20penalized%20regression.pdf

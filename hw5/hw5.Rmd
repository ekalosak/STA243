---
title: "STA243 Hw5"
author:
- Eric Kalosa-Kenyon
date: "`r format(Sys.time(), '%d %B %Y')`"

abstract: In this assignment, we fit a cubic penalized regression spline using four different methods for obtaining a smoothing parameter.

output:
  html_document:
    toc: yes
---

# Simulation setup

For my simulations, I consider the goodness of fit of cubic penalized regression
splines to the 1-dimensional function $f(x)$. A penalized regression spline is a
function of the form

$$
\hat{f}(x) = \sum_{i=1}^p \beta_i x^i +
    \sum_{j=1}^k \beta_{p_j}(x-t_j)^p_{+}
$$

Where $(x)_{+} = \textrm{max}(x, 0)$, $p$ is the degree of the spline, the
$t_j$ are the locations of the knots,
and $\beta = [\beta_0, \dots, \beta_{p_k}]$ are the regression coefficients. I
solve for the $\beta$ coefficients by minimizing a penalized functional shown
later in the report - this is what makes the spline "penalized". Using $k<n$
knots makes the spline a "regression" rather than "smoothing". I fit a spline to
a target function $f(x)$, but the spline may not be too overfit and it must have
$k<<n$ homogenously collocated knots - both conditions will be discussed in more
detail and formality in following sections.

An example target function $f(x)$ follows, and I will use it repeatedly to
contextualize theoretical statements throughout this report.

$$
f(x) = 1.5*\Phi(\frac{x-0.35}{0.15}) - \Phi(\frac{x-0.8}{0.04})
\quad \textrm{where} \quad
\Phi(u) = \frac{1}{\sqrt{2\pi}} e^{-u^2/2}
$$

```{r}
## Imports
library(ggplot2)
library(stats)
library(reshape)
library(latex2exp)

## Subroutines
f = function(x){
    # prob density function defined in problem specification
    1.5*dnorm((x-0.35)/0.15) - dnorm((x-0.8)/0.04)
}

## Parameterize
n = 200
is = 1:n
xis = (is-0.5)/n

## Plotting
pdf_df = data.frame(x=xis, y=f(xis))
plt_pdf = ggplot(data=pdf_df) +
    geom_line(aes(x=x, y=y), color="steelblue") +
    labs(x="X", y="Y", title="True f(x)")
```

```{r echo=FALSE}
plt_pdf
```

# Introduction
In this report, I simulate six different methods for selecting a penalty
parameter $\lambda$ under six different noise, design density, spatial variation,
and variance heterogeneity states.
The four aforementioned methods for fitting the penalty parameter
$\lambda$ for a penalized cubic regression spline are minimizations of the
following scores: cross validation, generalized cross validation, corrected
Akaike information criterion, and expected risk.

In this section, I introduce, the concepts required for the simulation in the
context of a simple example: $y=f(x)+\epsilon$. For the rest of this section,
the reader may assume, unless I state otherwise, that $f(x)$ is the example
function defined in the previous section.

To fit a cubic penalized regression spline $\hat{f}_\lambda(x)$ which
approximates $f(x)$, I minimize the following functional within the class
$\mathbf{F}$ of cubic regression splines:

$$
\hat{f}_\lambda(x) =
\textrm{argmin}_{f\in\mathbf{F}} \sum_{i=1}^{n}(y_i-f(x_i))^2 +
    \lambda\sum_{j=1}^{k} \beta_{p_j}^2 =
||y-f(x)||_2^2 + \lambda\beta^\top D\beta
$$

where $D$ is the diagonal matrix with $p+1$ $0$s followed by $K$ $1$s.

As David Ruppert, M. P. Wand, and R. J. Carroll (Semiparametric Regression 2003)
show, the solution to minimizing the functional above follows the form

$$
\hat{f}_\lambda(x) = x(x^\top x+\lambda D)^{-1} x^\top y = H_\lambda y
$$

Notice that this is effectively ridge regression on the knot coefficients and
ordinary least squares on the non-knot coefficients. The following `R` code
calculates the matrix $H_\lambda$, generates noisy
observations from the previously defined example target function $f(x)$, and
fits penalized cubic regression splines for a number of evocative values of
$\lambda$.

```{r}
## Subroutines
noise_sd = function(j){
    # varaible noise schedule
    return(0.02+0.04*(j-1)^2)
}

## Parameterize
J = 6 # for generating noisy observations
k = 30 # number of knots
p = 3 # cubic spline
a = 0
b = 1 # bounds of inegration for lambda*int_a^b f^''(x)^2 dx penalty
knots = seq(a, b, length.out=k)

fxs = f(xis)
noise_df = data.frame(x=xis, yt=fxs)
for(j in 1:J){
    yis = fxs + rnorm(n, mean=0, sd=noise_sd(j))
    colname = paste("y", j, sep="")
    noise_df[colname] = yis
}

# Perform cubic penalized spline regression for multiple lambda values
ex_xs = noise_df$x
ex_ys = noise_df$y3 # arbitrary noise profile for example
lam_df = data.frame(x=xis, y=fxs, yn=ex_ys)
lams = 10^seq(-10, 5, length.out=5)

D = diag(c(rep(0, p+1), rep(1, k)))
X1 = outer(ex_xs, 0:p, "^")
X2 = outer(ex_xs, knots, ">")*outer(ex_xs, knots, "-")^p # source [2]
X = cbind(X1, X2)
Y = ex_ys

for(lam in lams){
    H = X %*% solve(t(X) %*% X + lam*D) %*% t(X)
    fhys = H %*% Y
    colname = paste("lambda", lam, sep="=")
    lam_df[colname] = fhys
}

plt_mx = ggplot(data=lam_df) +
    geom_line(data=melt(lam_df, id=c("x", "y", "yn")),
              aes(x=x, y=value, color=variable)) +
    geom_point(aes(x=x, y=yn), alpha=0.4, shape=18, size=1.5) +
    labs(x="X", y="Y",
        title=paste(
            "Example cubic penalized regression spline with", k, "knots"
        )
    ) +
    scale_colour_discrete(
        labels=as.character(lams),
        name=TeX("Penalty $\\lambda$")
    )
plt_mx
```

# Penalty parameter selection
In this report, I am concerned with the selection of the penalty parameter
$\lambda$.
As stated in the introduction, I consider four methods for selecting the
optimal penalty parameter $\lambda$. In this section, I describe and perform
cross validation, generalized cross validation, improved AIC, and $L_2$ risk
minimization on a single experimental simulation setting. In the following
section, I perform these methods on a variety of experimental simulation
settings, but I've simplified this section to expose the methodology to the
reader.

## Cross validation (CV)

I am interested in finding the value of $\lambda$ that minimizes the following
function for a particular sample $\{x_i,y_i\}$. I accomplish this with a simple
grid search over an appropriate range of $\lambda>0$.

$$
CV(\lambda) = \frac{1}{N}\sum_{i=1}^{N}
    (\frac{y_i-\hat{f}_\lambda(x_i)}{1-(S_\lambda)_{i,i}})^2
$$

```{r}
cv = function(l, k=k, X=X, Y=Y){
    # k is num knots
    # l is lambda, penalty
    # X is design matrix
    # Y is response

    n = dim(X)[1]
    D = diag(c(rep(0, dim(X)[2]-k), rep(1, k)))
    H = X %*% solve(t(X) %*% X + l*D) %*% t(X)
    fhys = H %*% Y
    hii = diag(H)
    r = mean(((Y-fhys)/(1-hii))^2)
    return(r)
}

cvv = Vectorize(cv, vectorize.args = c("l"))
lambdas = 10^(seq(-10, -4, length.out=100))
cvs = cvv(X=X, Y=Y, l=lambdas, k=k)

cv_df = data.frame(x=lambdas, y=cvs)
plt_cv = ggplot(data=cv_df) +
    geom_line(aes(x=x, y=y), color="steelblue") +
    labs(x="Smoothing Penalty", y="Cross Validation Score",
         title="Cross validated values of lambda") +
    scale_x_log10() +
    # geom_hline(yintercept = min(cvs), color="coral") +
    geom_vline(xintercept = lambdas[which(cvs == min(cvs))], color="coral")
```

```{r echo=F}
lcv = lambdas[which(cvs == min(cvs))]
plt_cv
```

Note that the minimizing smoothing penalty is
$\lambda_{CV}=`r lcv`$
with a cross validation score of
$CV(\lambda_{CV})=`r min(cvs)`$.

## Generalized cross validation (GCV)

As in the preceeding subsection, I am interested in finding the value of
$\lambda$ that minimizes the following function for a particular sample
$\{x_i,y_i\}$. I accomplish this with a simple grid search over an appropriate
range of $\lambda>0$.

$$
GCV(\lambda) = \frac{\frac{1}{N}\sum_{i=1}^{N}
    (y_i-\hat{f}_\lambda(x_i))^2}{(1-\frac{1}{N}\textrm{tr}(S_\lambda))^2}
$$

```{r}
gcv = function(X=X, Y=Y, k=k, l){
    # k is num knots
    # l is lambda, penalty
    # X is design matrix
    # Y is response

    n = dim(X)[1]
    D = diag(c(rep(0, dim(X)[2]-k), rep(1, k)))
    H = X %*% solve(t(X) %*% X + l*D) %*% t(X)

    fhys = H %*% Y
    tr = diag(H)

    numer = mean((Y-fhys)^2)
    denom = (1-mean(tr))^2
    r = numer/denom
    return(r)
}

gcvv = Vectorize(gcv, vectorize.args = c("l"))
lambdas = 10^(seq(-9, -4, length.out=100))
gcvs = gcvv(X=X, Y=Y, l=lambdas, k=k)

gcv_df = data.frame(x=lambdas, y=gcvs)
plt_gcv = ggplot(data=gcv_df) +
    geom_line(aes(x=x, y=y), color="steelblue") +
    labs(x="Smoothing Penalty", y="Generalized Cross Validation Score",
         title="Generalized cross validated values of lambda") +
    scale_x_log10() +
    geom_vline(xintercept = lambdas[which(gcvs == min(gcvs))], color="coral")
```

```{r echo=F}
plt_gcv
```

Note that the minimizing smoothing penalty is
$\lambda_{GCV}=`r lambdas[which(gcvs == min(gcvs))]`$
with a generalized cross validation score of
$GCV(\lambda_{GCV})=`r min(gcvs)`$.

## Akaike information criterion (AIC)

As in the preceeding subsection, I am interested in finding the value of
$\lambda$ that minimizes the following function for a particular sample
$\{x_i,y_i\}$, and I use the same methods to find it.

$$
AIC_C(\lambda) = \textrm{log}(||y-\hat{f}_\lambda||_2^2) +
    \frac{2(\textrm{tr}(H_\lambda)+1)}{n-\textrm{tr}(H_\lambda)-2}
$$

```{r}
aicc = function(X=X, Y=Y, k=k, l){
    # k is num knots
    # l is lambda, penalty
    # X is design matrix
    # Y is response

    n = dim(X)[1]
    D = diag(c(rep(0, dim(X)[2]-k), rep(1, k)))
    H = X %*% solve(t(X) %*% X + l*D) %*% t(X)

    fhys = H %*% Y
    tr = sum(diag(H))

    r1 = log(mean((Y-fhys)^2))
    r2n = 2*(tr+1)
    r2d = n - tr - 2
    r = r1 + r2n/r2d
    return(r)
}

aiccv = Vectorize(aicc, vectorize.args = c("l"))
lambdas = 10^(seq(-8, -3, length.out=100))
aiccs = aiccv(X=X, Y=Y, l=lambdas, k=k)

aicc_df = data.frame(x=lambdas, y=aiccs)
plt_aicc = ggplot(data=aicc_df) +
    geom_line(aes(x=x, y=y), color="steelblue") +
    labs(x="Smoothing Penalty", y="Corrected AIC Score",
         title="Corrected AIC scores for smoothing parameter") +
    scale_x_log10() +
    geom_vline(xintercept = lambdas[which(aiccs == min(aiccs))], color="coral")
```

```{r echo=F}
plt_aicc
```

Note that the minimizing smoothing penalty is
$\lambda_{AIC_C}=`r lambdas[which(aiccs == min(aiccs))]`$
with a score of
$AIC_C(\lambda_{AIC_C})=`r min(aiccs)`$.

## Emperical risk minimization (ERM)

In trying to minimize $risk(\lambda) = E(||f-\hat{f}_\lambda||^2)$, I actually
minimize an estimate of this quantity because the true $f$ is inaccessable by
assumption. I need a related quantity to serve as an estimate.

First, I prove that
$E(||y-\hat{f}_\lambda||^2) = ||(I-H_\lambda)f||^2 +
    \sigma^2(tr(H_\lambda H_\lambda^\top) -2tr(H_\lambda) + n)$
to get a sense for what this estimate will look like.

1. $E(||y-\hat{f}_\lambda||^2) = E(||(I-H_\lambda)y||^2)$ because
$\hat{f}_\lambda = H_\lambda y$ and real matricies have both left and right
distributivity.

2. $E(||(I-H_\lambda)y||^2) = E(||(I-H_\lambda)(f + e)||^2)$ because $y=f+e$.

3. $E(||(I-H_\lambda)(f + e)||^2) =
E(||(I-H_\lambda)f||^2 + ||(I-H_\lambda)e)||^2)$ because $E(e)=0$ by
assumpion and the distributivity property noted in (1).

4. $E(||(I-H_\lambda)f||^2 + ||(I-H_\lambda)e||^2) =
||(I-H_\lambda)f||^2 + E(||(I-H_\lambda)e)||^2$ because the lefthand quantity is
a constant.
It remains to show that $E(||(I-H_\lambda)e||^2) =
\sigma^2(tr(H_\lambda H_\lambda^\top) -2tr(H_\lambda) + n)$

5. $E(||(I-H_\lambda)e||^2) =
\sigma^2tr(I (H_\lambda-I)(H_\lambda-I)^\top I^\top)$ because
$Cov(e) = I\sigma^2$.

6. $tr(I (H_\lambda-I)(H_\lambda-I)^\top I^\top) = \sum_{i=1}^n (h_{ii}-1)^2$.

7. $\sum_{i=1}^n (h_{\lambda,ii} - 1)^2 =
   tr(H_\lambda H_\lambda^\top) -2tr(H_\lambda) + n$

completing the proof.

In the following steps, I develop an estimator for
$risk(\lambda) = E(||f-\hat{f}_\lambda||^2)$:

1. $risk(\lambda) = E(||y-e-\hat{f}_\lambda||^2)$ by the model assumption
   $y=f+e$.
2. Expanding the $L_2$ norm in the expression above, using the linearity of
   expectation, and noting the fact that
   $E(e)=0$;
   $E(||y-e-\hat{f}_\lambda|||^2) =
    E(y^\top y - y^\top \hat{f}_\lambda + e^\top e - \hat{f}_\lambda^\top y +
    \hat{f}_\lambda^\top \hat{f}_\lambda)$
3. Consolidating terms in (2), $risk(\lambda) =
   n\sigma^2 + E(||y-\hat{f}_\lambda||^2)$ and we use the method of moments to
   create an unbiased estimator when $\sigma^2$ is known.

$y$ are the noisy observations, $\hat{f}_\lambda = H_\lambda y$ where
$H_\lambda$ depends only on $X$, and we can create an estimator for $\sigma^2$:
$\hat{\sigma^2} \approx RSS(\hat{f}_\lambda) =
\frac{1}{n} ||y-\hat{f}_\lambda||^2$. Using this estimator for $\sigma^2$,
the estimator for $E(||f-\hat{f}_\lambda||^2)$ is proportional to
$||y-\hat{f}_\lambda||^2$.

An alternative estimator for the risk, using classical pilots, is given in
section 2.2.1 of Lee (2003):
use $\hat{f}_{\lambda_p} \approx f$ where $\lambda_p$ is the $CV$ parameter.
This is the method I employ in my algorithm.

The following algorithm exhibits this risk estimator minimization criterion for
calculating the penalty parameter $\lambda$ for a single experimental simulation
parameterization.

```{r}
sighat = function(Y=Y, X=X, k=k, l){
    D = diag(c(rep(0, dim(X)[2]-k), rep(1, k)))
    H = X %*% solve(t(X) %*% X + l*D) %*% t(X)
    fhys = H %*% Y
    r = mean((Y-fhys)^2)
    return(r)
}

er = function(X=X, Y=Y, k=k, l, lp){
    # k is num knots
    # l is lambda, penalty
    # X is design matrix
    # Y is response

    n = dim(X)[1]
    D = diag(c(rep(0, dim(X)[2]-k), rep(1, k)))
    H = X %*% solve(t(X) %*% X + l*D) %*% t(X)
    Hcv = X %*% solve(t(X) %*% X + lp*D) %*% t(X)

    fcvhys = Hcv %*% Y
    fhys = H %*% Y
    sig = sighat(X=X, Y=Y, k=k, l=lp)

    I = diag(dim(H)[1])
    r = (1/n)*(sum(((H-I)%*%fcvhys)^2) + sig*sum(diag(H%*%t(H))))
    return(r)
}

erv = Vectorize(er, vectorize.args = c("l"))
lambdas = 10^(seq(-10, -3, length.out=100))
ers = erv(X=X, Y=Y, l=lambdas, k=k, lp=lcv)

er_df = data.frame(x=lambdas, y=ers)
plt_er = ggplot(data=er_df) +
    geom_line(aes(x=x, y=y), color="steelblue") +
    labs(x="Smoothing Penalty", y="Estimated Risk",
         title="Expected risk for smoothing parameter") +
    scale_x_log10() +
    geom_vline(xintercept = lambdas[which(ers == min(ers))], color="coral")
```

```{r echo=F}
plt_er
```

Note that the minimizing smoothing penalty is
$\lambda_{ER}=`r lambdas[which(ers == min(ers))]`$
with a score of
$ER(\lambda_{ER})=`r min(ers)`$.

# Comparison of methods for smoothing parameter selection

Now that I've shown the basic behavior of the four penalty parameter estimation
methods, I will examine their performance in a number of simulation settings.

## Performance under different levels of noise
Here I evaluate the performance of $\lambda$ estimators under the model
$y_{ij} = f(x_i) + \sigma_j\epsilon_i$ where $\sigma_j= 0.02 + 0.04(j − 1)^2$
and $\epsilon_i \sim\textrm{iid}N(0,1)$.

```{r}
## Parameterize
J = 6 # number of simulation parameterizations
M = 20 # number of simulations
L = 150 # number of lambdas to grid search
p = 3 # dimension of spline
k = 30 # number of knots
n = 200 # number of observations
a = 0
b = 1
knots = seq(a, b, length.out=k)
xs = ((1:n)-0.5)/n
fxs = f(xs)
lambdas = 10^seq(-10,0, length.out=L)

## Setup
# Data storage objects
noise_raw_df = data.frame(x=xs, yt=fxs) # observations for plotting
noise_lam_df = data.frame( # best lambda for each simulation 1:M
    cv=rep(NA, J*M), gcv=rep(NA, J*M), aic=rep(NA, J*M), er=rep(NA, J*M),
    J=rep(NA, J*M))
noise_sco_df = data.frame( # score of the corresponding best lambda
    cv=rep(NA, J*M), gcv=rep(NA, J*M), aic=rep(NA, J*M), er=rep(NA, J*M),
    J=rep(NA, J*M))

D = diag(c(rep(0, p+1), rep(1, k)))
X1 = outer(xs, 0:p, "^")
X2 = outer(xs, knots, ">")*outer(xs, knots, "-")^p # source [2]
X = cbind(X1, X2)

for(j in 1:J){

    for(i in 1:M){
        # generate observatons
        Y = fxs + rnorm(n, mean=0, sd=noise_sd(j))

        # calculate lambda using each method
        cvs = cvv(k=k, l=lambdas, X=X, Y=Y)
        gcvs = gcvv(k=k, l=lambdas, X=X, Y=Y)
        aiccs = aiccv(k=k, l=lambdas, X=X, Y=Y)

        bcv = min(cvs) # best cv score is lowest
        cv_lam = lambdas[which(cvs == bcv)]

        ers = erv(k=k, l=lambdas, X=X, Y=Y, lp=cv_lam) # need cv_lam for pilot

        bgcv = min(gcvs)
        gcv_lam = lambdas[which(gcvs == bgcv)]
        baicc = min(aiccs)
        aicc_lam = lambdas[which(aiccs == baicc)]
        ber = min(ers)
        er_lam = lambdas[which(ers == ber)]

        ix = (j-1)*M + i
        noise_lam_df$J[ix] = j
        noise_lam_df$cv[ix] = cv_lam
        noise_lam_df$gcv[ix] = gcv_lam
        noise_lam_df$aic[ix] = aicc_lam
        noise_lam_df$er[ix] = er_lam

        noise_sco_df$cv[ix] = bcv
        noise_sco_df$gcv[ix] = bgcv
        noise_sco_df$aic[ix] = baicc
        noise_sco_df$er[ix] = ber

    }

    # record an example of the observations generated in this noise regime
    colname = paste("J=", j, sep="")
    noise_raw_df[colname] = Y
}

## Plot results
plt_noise_obv = ggplot(data=melt(noise_raw_df, id=c("x", "yt"))) +
    geom_line(data=noise_raw_df, aes(x=x, y=yt), color="steelblue") +
    geom_point(aes(x=x, y=value, color=variable),
               alpha=0.7, size=1.5, shape=18) +
    facet_wrap(~variable) +
    scale_colour_discrete(
        labels=as.character(1:J),
        name="Noise regime (J)"
    ) +
    labs(x="X", y="Y", title="Observations with different noise")

noise_lam_df_m = melt(noise_lam_df, id=c("J"))
noise_lam_df_m$J = as.factor(noise_lam_df_m$J)

ylims = boxplot.stats(noise_lam_df_m$value)$stats[c(1, 5)]

plt_noise_lams = ggplot(data=noise_lam_df_m) +
    geom_boxplot(aes(x=J, y=value, color=J), outlier.shape=NA) +
    coord_cartesian(ylim = ylims*1.05) +
    facet_wrap(~variable) +
    scale_y_log10() +
    scale_colour_discrete(
        labels=as.character(1:J),
        name="Noise regime (J)"
    ) +
    labs(x="Noise regime", y=TeX("Optimal $\\lambda$"),
         title=TeX("Stability of different methods for estimating $\\lambda$"))
```

```{r echo=F}
plt_noise_obv
```
  
The above plot illustrates the effects of noise regimes $j\in\{1,\dots,J=6\}$.

```{r echo=F}
plt_noise_lams
```
  
As the level of noise increases, the optimal penalty parameter increases and its
stability decreases. The increase in $\lambda$ means that I encourage the spline
to be smoother - to fit less tightly to the noisy data - as the level of noise
increases.

## Performance under different design density
In this section, I examine the effect of different densities of $x_i$ values on
the four different penalty parameter selection methods. I use the following
model as in Lee (2003):

$$
y_{ij} = f(X_{ij}) + \sigma\epsilon_i \quad \textrm{where} \quad
\sigma = 0.1, X_{ij} = F^{-1}(X_i),
F(X) = \textrm{Beta}(\frac{j+4}{5}, \frac{11-j}{5}), X_i\sim \textrm{Unif}(0,1)
$$


```{r}
## Parameterize
lambdas = 10^seq(-9,-4, length.out=L)

## Setup
# Data storage objects
xts = ((1:n)-0.05)/n
yts = f(xts)
dens_raw_df = data.frame(x=xts, y=yts, j=rep(0, n)) # observations for plotting
dens_lam_df = data.frame( # best lambda for each simulation 1:M
    cv=rep(NA, J*M), gcv=rep(NA, J*M), aic=rep(NA, J*M), er=rep(NA, J*M),
    J=rep(NA, J*M))
dens_sco_df = data.frame( # score of the corresponding best lambda
    cv=rep(NA, J*M), gcv=rep(NA, J*M), aic=rep(NA, J*M), er=rep(NA, J*M),
    J=rep(NA, J*M))

D = diag(c(rep(0, p+1), rep(1, k)))

for(j in 1:J){

    for(i in 1:M){

        # variable density Xs
        xs = qbeta(runif(n), (j+4)/5, (11-j)/5)
        fxs = f(xs)
        X1 = outer(xs, 0:p, "^")
        X2 = outer(xs, knots, ">")*outer(xs, knots, "-")^p # source [2]
        X = cbind(X1, X2)

        # generate observatons
        Y = fxs + rnorm(n, mean=0, sd=0.1)

        # calculate lambda using each method
        cvs = cvv(k=k, l=lambdas, X=X, Y=Y)
        gcvs = gcvv(k=k, l=lambdas, X=X, Y=Y)
        aiccs = aiccv(k=k, l=lambdas, X=X, Y=Y)

        bcv = min(cvs) # best cv score is lowest
        cv_lam = lambdas[which(cvs == bcv)]

        ers = erv(k=k, l=lambdas, X=X, Y=Y, lp=cv_lam) # need cv_lam for pilot

        bgcv = min(gcvs)
        gcv_lam = lambdas[which(gcvs == bgcv)]
        baicc = min(aiccs)
        aicc_lam = lambdas[which(aiccs == baicc)]
        ber = min(ers)
        er_lam = lambdas[which(ers == ber)]

        ix = (j-1)*M + i
        dens_lam_df$J[ix] = j
        dens_lam_df$cv[ix] = cv_lam
        dens_lam_df$gcv[ix] = gcv_lam
        dens_lam_df$aic[ix] = aicc_lam
        dens_lam_df$er[ix] = er_lam

        dens_sco_df$cv[ix] = bcv
        dens_sco_df$gcv[ix] = bgcv
        dens_sco_df$aic[ix] = baicc
        dens_sco_df$er[ix] = ber

    }

    # record an example of the observations generated in this noise regime
    tdf = data.frame(x=xs, y=Y, j=rep(j, n)) # observations for plotting
    dens_raw_df = rbind(dens_raw_df, tdf)
}

dens_raw_df$j = as.factor(dens_raw_df$j)
drdf0 = subset(dens_raw_df, j == 0, select=c("x","y"))
plt_dens_obv = ggplot(data=drdf0) +
    geom_line(aes(x=x,y=y), color="steelblue") +
    geom_point(data=subset(dens_raw_df, j != 0), aes(x=x, y=y, color=j),
               shape=18, size=1.5) +
    facet_wrap(~j) +
    scale_colour_discrete(
        labels=as.character(1:J),
        name="Support variance regime (J)"
    ) +
    labs(x="X", y="Y", title="Observations with variable support density")

dens_lam_df_m = melt(dens_lam_df, id=c("J"))
dens_lam_df_m$J = as.factor(dens_lam_df_m$J)
ylims = boxplot.stats(dens_lam_df_m$value)$stats[c(1, 5)]
plt_dens_lams = ggplot(data=dens_lam_df_m) +
    geom_boxplot(aes(x=J, y=value, color=J), outlier.shape=NA) +
    scale_y_log10() +
    coord_cartesian(ylim = ylims*1.05) +
    facet_wrap(~variable) +
    scale_colour_discrete(
        labels=as.character(1:J),
        name="Support variance regime (J)"
    ) +
    labs(x="Support variance regime", y=TeX("Optimal $\\lambda$"),
         title=TeX("Stability of different methods for estimating $\\lambda$"))
```

```{r echo=F}
plt_dens_obv
```
  
As illustrated above, the small values of $j$ group points to the lefthand side
of the support, while the large values of $j$ group points to the righthand
side.

```{r echo=F}
plt_dens_lams
```
  
The stability of the penalty parameter estimations is much lower when the points
are not homogenously dense thoughout the support. This is especially true
considering I'm fitting a spline with `r k` equally spaced knots. A variable
knot seletion protocol might mitigate some of the instability of the penalty
parameter estimates observed for $j\in\{1,2,5,6\}$.

## Performance under different frequency of target function
In this section I examine the effects of spatial varaition on the stability of
the penalty parameter $\lambda$. I use the following model:

$$
y_{ij} = f_j(x_{i}) + \sigma\epsilon_i \quad \textrm{where} \quad
\sigma = 0.2,
f_j(x) = \sqrt{x*(1-x)}\textrm{sin}(\frac{2\pi(1+2^{(9-4j)/5})}{
x+2^{(9-4j)/5}}), \epsilon_i\sim \textrm{iid}N(0,1)
$$

```{r}
## Parameterize
xs = ((1:n)-0.5)/n
lambdas = 10^seq(-11,-1, length.out=L)

## Subroutines
ff = function(x, j){
    r = sqrt(x*(1-x))*sin((2*pi*(1+2^((9-4*j)/5)))/(x+2^((9-4*j)/5)))
    return(r)
}

## Setup
# Data storage objects
sp_raw_df = data.frame(x=NA, yt=NA, ys=NA, j=NA) # observations for plotting
sp_lam_df = data.frame( # best lambda for each simulation 1:M
    cv=rep(NA, J*M), gcv=rep(NA, J*M), aic=rep(NA, J*M), er=rep(NA, J*M),
    J=rep(NA, J*M))

D = diag(c(rep(0, p+1), rep(1, k)))
X1 = outer(xs, 0:p, "^")
X2 = outer(xs, knots, ">")*outer(xs, knots, "-")^p # source [2]
X = cbind(X1, X2)

for(j in 1:J){

    fxs = ff(xs, j)

    for(i in 1:M){

        # generate observatons
        Y = fxs + rnorm(n, mean=0, sd=0.2)

        # calculate lambda using each method
        cvs = cvv(k=k, l=lambdas, X=X, Y=Y)
        gcvs = gcvv(k=k, l=lambdas, X=X, Y=Y)
        aiccs = aiccv(k=k, l=lambdas, X=X, Y=Y)

        bcv = min(cvs) # best cv score is lowest
        cv_lam = lambdas[which(cvs == bcv)]

        ers = erv(k=k, l=lambdas, X=X, Y=Y, lp=cv_lam) # need cv_lam for pilot

        bgcv = min(gcvs)
        gcv_lam = lambdas[which(gcvs == bgcv)]
        baicc = min(aiccs)
        aicc_lam = lambdas[which(aiccs == baicc)]
        ber = min(ers)
        er_lam = lambdas[which(ers == ber)]

        ix = (j-1)*M + i
        sp_lam_df$J[ix] = j
        sp_lam_df$cv[ix] = cv_lam
        sp_lam_df$gcv[ix] = gcv_lam
        sp_lam_df$aic[ix] = aicc_lam
        sp_lam_df$er[ix] = er_lam

    }

    # record an example of the observations generated in this noise regime
    tdf = data.frame(x=xs, yt=fxs, ys=Y, j=rep(j, n)) # observations for plotting
    sp_raw_df = rbind(sp_raw_df, tdf)
}

sp_raw_df = subset(sp_raw_df, ! is.na(j)) # remove the NA row
sp_raw_df$j = as.factor(sp_raw_df$j)
plt_sp_obv = ggplot(data=sp_raw_df) +
    geom_line(aes(x=x,y=yt), color="steelblue") +
    geom_point(aes(x=x, y=ys, color=j), shape=18, size=1.5) +
    facet_wrap(~j) +
    scale_colour_discrete(
        labels=as.character(1:J),
        name="Function frequency regime (J)"
    ) +
    labs(x="X", y="Y",
        title="Observations generated by different target function frequencies")

sp_lam_df_m = melt(sp_lam_df, id=c("J"))
sp_lam_df_m$J = as.factor(sp_lam_df_m$J)
plt_sp_lams = ggplot(data=sp_lam_df_m) +
    geom_boxplot(aes(x=J, y=value, color=J)) +
    scale_y_log10() +
    facet_wrap(~variable) +
    scale_colour_discrete(
        labels=as.character(1:J),
        name="Function frequency regime (J)"
    ) +
    labs(x="Target function frequencies", y=TeX("Optimal $\\lambda$"),
         title=TeX("Stability of different methods for estimating $\\lambda$"))
```

```{r echo=F}
plt_sp_obv
```
  
As illustrated above, increasing $j$ increases the frequency of the spatial
distribution function $f(x)$.

```{r echo=F}
plt_sp_lams
```
  
Interestingly, increased spatial frequency stabilizes the estimates of the
penalty parameter to a point. Furthermore, as the function becomes more erratic,
the value of the penalty parameter $\lambda$ decreases, forcing the spline to
hug the data more closely.

## Performance under heterogeneous variances
In this section I examine the effects of spatial variance dependence on the
stability of estimates of the penalty parameter $\lambda$. I use the following
model:

$$
y_{ij} = f(x_{i}) + \sqrt{v_j(x_i)}\epsilon_i \quad \textrm{where} \quad
v_j(x) = (0.15(1 + 0.4(2j-7)(x-0.5)))^2
$$
and $f(x)$ and $\epsilon_i$ are defined as in the introduction.

```{r}
## Parameterize
xs = ((1:n)-0.5)/n
lambdas = 10^seq(-11,-1, length.out=L)

## Subroutines
fv = function(x, j){
    r = (0.15*(1+0.4*(2*j-7)*(x-0.5)))^2
    return(sqrt(r))
}

## Setup
# Data storage objects
v_raw_df = data.frame(x=NA, yt=NA, ys=NA, j=NA) # observations for plotting
v_lam_df = data.frame( # best lambda for each simulation 1:M
    cv=rep(NA, J*M), gcv=rep(NA, J*M), aic=rep(NA, J*M), er=rep(NA, J*M),
    J=rep(NA, J*M))

D = diag(c(rep(0, p+1), rep(1, k)))
X1 = outer(xs, 0:p, "^")
X2 = outer(xs, knots, ">")*outer(xs, knots, "-")^p # source [2]
X = cbind(X1, X2)
fxs = f(xs)

for(j in 1:J){

    for(i in 1:M){

        # generate observatons
        Y = fxs + fv(xs, j)*rnorm(n, mean=0, sd=1)

        # calculate lambda using each method
        cvs = cvv(k=k, l=lambdas, X=X, Y=Y)
        gcvs = gcvv(k=k, l=lambdas, X=X, Y=Y)
        aiccs = aiccv(k=k, l=lambdas, X=X, Y=Y)

        bcv = min(cvs) # best cv score is lowest
        cv_lam = lambdas[which(cvs == bcv)]

        ers = erv(k=k, l=lambdas, X=X, Y=Y, lp=cv_lam) # need cv_lam for pilot

        bgcv = min(gcvs)
        gcv_lam = lambdas[which(gcvs == bgcv)]
        baicc = min(aiccs)
        aicc_lam = lambdas[which(aiccs == baicc)]
        ber = min(ers)
        er_lam = lambdas[which(ers == ber)]

        ix = (j-1)*M + i
        sp_lam_df$J[ix] = j
        sp_lam_df$cv[ix] = cv_lam
        sp_lam_df$gcv[ix] = gcv_lam
        sp_lam_df$aic[ix] = aicc_lam
        sp_lam_df$er[ix] = er_lam

    }

    # record an example of the observations generated in this noise regime
    tdf = data.frame(x=xs, yt=fxs, ys=Y, j=rep(j, n)) # observations for plotting
    v_raw_df = rbind(v_raw_df, tdf)
}

v_raw_df = subset(v_raw_df, ! is.na(j)) # remove the NA row
v_raw_df$j = as.factor(v_raw_df$j)
plt_v_obv = ggplot(data=v_raw_df) +
    geom_line(aes(x=x,y=yt), color="steelblue") +
    geom_point(aes(x=x, y=ys, color=j), shape=18, size=1.5) +
    facet_wrap(~j) +
    scale_colour_discrete(
        labels=as.character(1:J),
        name="Variance regime (J)"
    ) +
    labs(x="X", y="Y",
        title="Observations generated by different variance structures")

v_lam_df_m = melt(sp_lam_df, id=c("J"))
v_lam_df_m$J = as.factor(v_lam_df_m$J)
plt_v_lams = ggplot(data=v_lam_df_m) +
    geom_boxplot(aes(x=J, y=value, color=J)) +
    scale_y_log10() +
    facet_wrap(~variable) +
    scale_colour_discrete(
        labels=as.character(1:J),
        name="Variance regime (J)"
    ) +
    labs(x="Variance structures", y=TeX("Optimal $\\lambda$"),
         title=TeX("Stability of different methods for estimating $\\lambda$"))
```

```{r echo=F}
plt_v_obv
```
  
As illustrated above, this method for varying the variance spacially incurs a
high variance for the lefthand side when $j$ is small and a high variance for
the righthand side when $j$ is large.

```{r echo=F}
plt_v_lams
```
  
Spatial varaince modulation destabilizes smoothing parameter estimates. Having
`r k` equispaced knots is a serious issue here and a more dynamic collocation
proceedure would ameliorate some of this instability in the $\lambda$ estimates.

# Conclusions
In this report, I performed a simulation study following Lee (2003) on the
behavior of penalty parameter $\lambda$ estimates under different target
function $f(x)$ and noise $\sigma\epsilon$ conditions. For each simulation
parameterization, there is at least one major qualitative take-away for the
pragmatic statistician, and these useful observations follow:

1. Higher noise incurs a higher penalty parameter. Essentially, this means that
   as the obervations deviate further and further from the target function, I
   give the spline a looser fit to the data. That is, when noise is high, I
   avoid overfitting.
2. When observation density is heterogeneous, knot collocation strategies become
   increasingly important. I observed a reduction in penalty parameter
   estimation when the observation density was highly skewed, especially when it
   was skewed away from portions of the target function with large higher order
   derivatives.
3. The more erratic the target function, the lower the penalty will be. That is,
   when the target function is more wiggly, I'll use a lower penalty to force
   the spline to fit closer to the additional curvature. As expected, the larger
   the higher order derivatives of the target function, the lower the stability
   of the penalty estimates across the board. Interestingly, when the curve is
   too linear, the penalty parameter estimates destabilize slightly - this
   behavior is worth further investigation.
4. High variance around nonlinearities in the target function induces
   instability in the penalty estimate. This is intuitive: I need more data
   around the complicated parts of the curve to get a good estimate.

Moving forward, theoretical results relating the behavior of these penalty
parameter estimates to qualities of the target function and noise need to be
developed.

# Sources
1. http://data.princeton.edu/eco572/smoothing.pdf
2. http://people.stat.sfu.ca/~cschwarz/Consulting/Trinity/Phase2/TrinityWorkshop/Workshop-handouts/TW-04-Intro-splines.pdf
3. Smoothing parameterselection forsmoothing splines: a simulation study
   Thomas C.M. Lee
   (Computational Statistics & Data Analysis 42 (2003) 139 – 148)
4. http://staff.ustc.edu.cn/~zwp/teach/nonpar/Spline%20and%20penalized%20regression.pdf

---
title: "STA243 Hw1 - Eric Kalosa-Kenyon"
output:
  html_document:
    toc: yes
---

```{r, message=F, warning=F}
library("ggplot2")          # fancy plotting
library("latex2exp")        # latex for plots
```

# 1
## 1.a
$p_n(x_1\dots x_n, \theta) = \prod_{i=1}^{n} p(x_i, \theta)$ where
$p(x, \theta) = \frac{1}{\pi*(1+(x-\theta)^2)}$. Hence,
$l_n(\theta) = \log{p_n(x_1\dots x_n, \theta)} =
    -n\log{\pi}-\sum_{i=1}^{n}{\log{1+(x_i-\theta)^2}}$
  
Further, $l_n^{'}(\theta) = \frac{\partial}{\partial \theta} l_n(\theta) =
    -\sum_{i=1}^{n} \frac{\partial}{\partial \theta} \log{1+(x_i-\theta)^2} =
    -2\sum_{i=1}^{n} \frac{\theta-x_i}{1+(x_i-\theta)^2}$.
  
Finally $l_n^{''}(\theta) = \frac{\partial^2}{\partial \theta^2} l_n(\theta) =
    \frac{\partial}{\partial \theta} l_n^{'}(\theta) =
    -2\sum_{i=1}^{n} \frac{1}{1+(x_i-\theta)^2} -
    (\theta-x_i)(1+(\theta-x_i)^2)^{-2}(2(\theta-x_i))$.
Rewriting some terms, we find
$l_n^{''}(\theta) =
    -2\sum_{i=1}^{n} \frac{1+(x_i-\theta)^2}{(1+(x_i-\theta)^2)^2} -
    \frac{-2(\theta-x_i)^2}{(1+(x_i-\theta)^2)^2}$
which simplifies to
$l_n^{''}(\theta) =
    -2\sum_{i=1}^{n} \frac{1-(\theta-x_i)^2}{(1+(\theta-x_i)^2)^2}$ as desired.


## 1.b
The Fischer Information is calculated as follows:
$I_n(\theta) = n*I(\theta)$.
$I(\theta) = -E_\theta(l^{''}(\theta)) =
    -\int_{-\infty}^{\infty} p(x,\theta)*l^{''}(\theta) dx$. This simplifies to
$\frac{2}{\pi} \int_{-\infty}^{\infty}
    \frac{1-(\theta-x)^2}{(1+(\theta-x)^2)^3} dx = \frac{2}{\pi} \frac{\pi}{4}$.
Leveraging the first equality, we see that $I_n(\theta) = \frac{n}{2}$.

## 1.c
```{r}
p_1 = function(x, t){
    d = pi*(1+(x-t)^2)
    return(1/d)
}
l_1 = function(x, t){
    return(log(p_1(x,t)))
}
l_n = function(xs, t){
    r = 0
    for(x in xs){
        r = r + l_1(x, t)
    }
    return(r)
}

N = 200
raw = c(-13.87, -2.53, -2.44, -2.40, -1.75, -1.34, -1.05, -0.23, -0.07, 0.27,
        1.77, 2.76, 3.29, 3.47, 3.71, 3.80, 4.24, 4.53, 43.21, 56.75)
ts = seq(from=min(raw)-10, to=max(raw)+10, length=N)

df = data.frame(ts, l_n(xs=raw, t=ts))
names(df) = c("theta", "loglik")

g = qplot(df$theta, df$loglik) +
    geom_line() +
    ggtitle(paste("Log likelihood collocated at", N, "points")) +
    ylab(TeX("l($\\theta)")) + xlab(TeX("$\\theta"))
g
```

## 1.d
```{r}
xis = c(-11, -1, 0, 1.4, 4.1, 4.8, 7, 8, 38) # initial values for NR
eps = 0.001 # convergence criterion
delt = 0.0001 # increment parameter for derivatives

f_pdf = function(t){
    # Log likelihood given raw data as xs
    return(l_n(xs=raw, t=t))
}
fp = function(f, t, d){
    # first derivative aprox
    f0 = f(t-d/2)
    f1 = f(t+d/2)
    return((f1-f0)/d)
}
fpp = function(f, t, d){
    # second derivative aprox
    return(fp(fp(f, t, d), t, d))
}

nr_inc = function(f, x, d){
    # Increment x using the newton rhapson method
    return(x-f(x)/fp(f, x, d))
}

find_0s = function(f, x0, e, d, inc_fxn){
    # use NR to find a zero for the function
    x1 = inc_fxn(f, x0, d)
    while(abs(x1-x0)>e){
        # While we haven't converged, iterate the incrementing function (which
        # must have the signature [pdf, x, delta]) e.g. nr_inc
        x0 = x1
        x1 = inc_fxn(f, x0, d)

        if(abs(x1)==Inf){
            # If the algorithm diverges, return Inf
            return(Inf)
        }
    }

    # If we've converged (abs(x1-x0)<e) then return the convergent value
    return(x1)
}

f_find_0 = function(t){
    # This is the function we're finding the 0's of
    # In this case, it's the first derivative of the log likelihood (f_pdf)
    return(fp(f_pdf, t, delt))
}

xmax = c() # holds results of NR algorithm, Inf when divergent
for(xi in xis){
    xmax = c(xmax, find_0s(f_find_0, xi, eps, delt, nr_inc))
}
```

The Newton-Rhapson method has found the following maxima for the initial values
$`r xis[xmax!=Inf]`$: $`r xmax[xmax!=Inf]`$.
The initial values $`r xis[xmax==Inf]`$ begat divergent results with the tuning
parameters used in this algorithm. These results show that the maximum found by
N-R is local and depends heavily on the initial conditions when the pdf is
multimodal as is the case in this example.

It is illustrative to examine the first derivative of the log likelihood
function for insight on divergence conditions:
```{r}
N = 200
xss = seq(-10,50,length.out=N)
yss = f_find_0(xss)
df = data.frame(xss, yss)
g = qplot(df$xss, df$yss) +
    geom_line() +
    ggtitle(
        paste("First derivative of log likelihood collocated at",
              N,
              "points")) +
    ylab(TeX("$\\partial l($\\theta)")) +
    xlab(TeX("$\\theta"))
g
```

Note that N-R finds the obvious roots of the first derivative of the log
likelihood function, but it diverges on the tails as it chases the 0's at
$\pm\infty$.

## 1.e
```{r}
fI = length(raw)/2  # I(\theta) = n/2 for the Cauchy dist.
fiscor_inc = function(f, x, d){
    # Increment x using the Fischer Scoring method
    return(x+f(x)/fI)
}
xmax = c() # holds results of FS algorithm, Inf when divergent
for(xi in xis){
    xmax = c(xmax, find_0s(f_find_0, xi, eps, delt, fiscor_inc))
}
```

The Fischer Scoring method has found the following maxima for the initial values
$`r xis[xmax!=Inf]`$: $`r xmax[xmax!=Inf]`$.
There were no divergent results using the Fischer Scoring algorithm in this
case, but it missed three apparent MLE candidate values. The MLE candidates
found with the Fischer Scoring method are in agreement with a subset of the
candidates proposed by the Newton-Rhapson method in 1.d above.


# 2
In this problem we examine the behavior of MLE convergence using the Newton
Rhapson method in the context of the pdf
$f_\theta(x) = \frac{1-\cos(x-\theta)}{2\pi}$ where
$0\le x \le 2\pi$ and $-\pi\le\theta\le\pi$.

# 2.a
```{r}
raw = c(0.52, 1.96, 2.22, 2.28, 2.28, 2.46, 2.50, 2.53, 2.54, 2.99, 3.47, 3.53,
        3.70, 3.88, 3.91, 4.04, 4.06, 4.82, 4.85, 5.46)

p_1 = function(x, t){
    # Single dimensional pdf of x with parameter t

    # Domain checking
    if(x<0 || x>2*pi){
        return(0)
    }
    if(t< -pi || t>pi){
        return(0)
    }

    return((1-cos(x-t))/(2*pi))
}

l_n = function(xs, t){
    # multidimensional log likelihood for the above pdf
    r = 0
    for(x in xs){
        r = r + log(p_1(x, t))
    }
    return(r)
}

llik = function(t){
    # log likelihood above conditioned on observing the raw data above
    return(l_n(raw, t))
}

# Plot the loglikelihood
N = 250
ts = seq(from=-pi, to=pi, length=N)

df = data.frame(ts, llik(ts))
names(df) = c("theta", "loglik")

rm(g)
g = qplot(df$theta, df$loglik) +
    geom_line() +
    ggtitle(paste("Log likelihood collocated at", N, "points")) +
    ylab(TeX("l($\\theta)")) + xlab(TeX("$\\theta"))
g
```

---
title: "STA243 Hw4"
author:
- Eric Kalosa-Kenyon
date: "`r format(Sys.time(), '%d %B %Y')`"
abstract: This assignment enforces Monte Carlo methods and exhibits MCMCs. We approximate integrals, perform importance sampling, etc.
output:
  html_document:
    toc: yes
runtime: shiny
---


# 1
## 1.a
$$\int_0^1 x^2 dx$$
We use $f(x)=x^2$ and $p(x)=1$ uniform over the $[0,1]$ interval.

```{r}
### R version 3.4.0 (2017-04-21) -- "You Stupid Darkness"
# x86_64-apple-darwin15.6.0 svn rev 72570

### BEGIN: Code

## Preamble
rm(list=ls())
set.seed(100)

## Imports
library(ggplot2)
library(latex2exp)

## Subroutines
f = function(x){
    return(x^2)
}
p = function(x){
    return(1)
}
f = Vectorize(f)
p = Vectorize(p)

## Parameterize
N = 5000 # Upper limit of samples to average for the MC integral estimation
m = 50 # Sample N/m, 2N/m, .., N times to observe convergence rate
K = 10000 # Resolution on the interval over which to sample w.r.t p(x)
v = 1 # int_0^1 dx = 1

## Simulate
pxs = seq(0, 1, length.out=K)
i1s = rep(NA, m)
ns = (1:m)*N/m
for(i in 1:m){ # m is small so this is fast enough
    xs = sample(pxs, ns[i], prob=p(pxs), replace=T) # sample according to p(x)
    i1 = v*mean(f(xs)) # sample mean of f(x)
    i1s[i] = i1
}

## Plot
df_plt1 = data.frame(x = ns, y = i1s)
plt1 = ggplot(data=df_plt1, aes(x=x, y=y)) +
    geom_line() +
    geom_hline(yintercept = 1/3) + # true evaluation
    labs(
         x="Samples",
         y="MC Estimation",
         title=TeX("MC Estimation of $\\int_0^1 x^2 dx$")
         )
plt1

### END: Code
```

The Monte Carlo estimation of $\int_0^1 x^2 dx$ is $\hat{I}=`r i1`$.
The exact evaluation is $I=\frac{x^3}{3}\bigg\rvert_0^1 = \frac{1}{3}$.
We are off by $\mid I-\hat{I}\mid=`r abs(i1-1/3)`$ which is only a factor of
$\frac{\mid I-\hat{I}\mid}{1/3}=`r abs(i1-1/3)/(1/3)`$.

## 1.b

$$\int_{-2}^2 \int_0^1 x^2 \cos(xy) dxdy$$
We use $f(x,y)=x^2\cos(xy)$ and $p(x,y)=\frac{1}{4}$ uniform over the
$[0,1]\times[-2,2]$ domain.

```{r}
### BEGIN: Code

## Subroutines
f = function(x, y){
    return(x^2*cos(x*y))
}
f = Vectorize(f)

## Parameterize
N = 5000 # Upper limit of samples to average for the MC integral estimation
m = 50 # Sample N/m, 2N/m, .., N times to observe convergence rate
Kx = 1000 # Resolution on the interval over which to sample w.r.t p(x)
Ky = 2000
v = 4 # int_0^1 int_-2^2 dxdy

## Simulate
pxs = seq(0, 1, length.out=Kx)
pys = seq(-2, 2, length.out=Ky)
i2s = rep(NA, m)
ns = (1:m)*N/m
for(i in 1:m){ # m is small so this is fast enough
    xs = sample(pxs, ns[i], replace=T) # sample uniformly
    ys = sample(pys, ns[i], replace=T)
    i2 = v*mean(f(xs, ys)) # sample mean of f(x)
    i2s[i] = i2
}

## Plot
tru = 1/2*(sin(2)-2*cos(2))
df_plt2 = data.frame(x = ns, y = i2s)
plt2 = ggplot(data=df_plt2, aes(x=x, y=y)) +
    geom_line() +
    geom_hline(yintercept = tru) + # true evaluation
    labs(
         x="Samples",
         y="MC Estimation",
         title=TeX(paste(
            "MC Estimation of",
            "$\\int_{-2}^2\\int_0^1 x^2 cos(xy) dxdy$"
            ))
         )
plt2

### END: Code
```

## 1.c

$$\int_{0}^{\infty} \frac{3}{4} x^4 e^{-x^3/4} dx$$
Notice that
$\int_0^{\infty} e^{-x^3/4} dx = 2^{2/3}\Gamma(\frac{4}{3}) = k$ so
we use
$$p(x) = \frac{1}{k}e^{-x^3/4}$$
and
$$f(x) = k\frac{3}{4}x^4$$
When substituted into the original integral, we see
$$
\int_{0}^{\infty} \frac{3}{4} x^4 e^{-x^3/4} dx =
\int_{0}^{\infty} f(x) dp(x)
$$

Hence, we sample from $p(x)$ using inverse CDF sampling and evaluate the samples
at $f(x)$ for a large number of samples following the MC integration method.
Before jumping into the code, however, $P(x)=\int_0^{x}p(y)dy$ must first be
calculated:
$$
P(x) = \int_0^{x}p(y)dy =
\frac{1}{k}(k-\frac{x}{3}E_{2/3}(\frac{y^3}{4}))
$$

where
$$
E_n(x)=\int_1^{\infty} \frac{e^{-xt}}{t^n} dt =
x^{n-1}\Gamma(1-n,x)
$$

and
$$
\Gamma(a,x) = \int_x^\infty t^a e^{-t} dt
$$

For convenience, we calculate the inverse CDF and the CDF numerically.

```{r}
### BEGIN: Code

## Imports
library(reshape)

## Parameterize
N = 10000 # Upper limit of samples to average for the MC integral estimation
m = 50 # Sample N/m, 2N/m, .., N times to observe convergence rate
K = 1000 # Resolution on the interval over which to sample w.r.t p(x)
v = 1 # int_0^inf dp(x)
mx = 4 # point after which p(mx) is practically 0

## Subroutines
k = 2^(2/3)*gamma(4/3)
f = function(x){
    return(k*3/4*x^4)
}
p = function(x){
    return(1/k*exp(-x^3/4))
}
f = Vectorize(f)
p = Vectorize(p)

## Precompute the CDF for inverse sampling
d = mx/K
pc_xs = seq(0, mx, by=d)
pdf = p(pc_xs)
cdf = cumsum(pdf)*d

# must calculate inverse cdf carefully so that it is uniformly sampled over 0,1
us = seq(0, 1, length.out=K) # uniform sampling grid
invcdf = rep(NA, K)
for(i in 1:length(us)){
    ix = sum(cdf<us[i]) + 1
    invcdf[i] = cdf[ix]
}

## Simulate
i3s = rep(NA, m)
ns = (1:m)*N/m

for(i in 1:m){ # m is small so this is fast enough
    xs = sample(pc_xs, ns[i], prob=pdf, replace=T) # sample uniformly
    i3 = v*mean(f(xs)) # sample mean of f(x)
    i3s[i] = i3
}

## Plot
# pdf and cdf
pc_df = data.frame(x=pc_xs, CDF=cdf, PDF=pdf)
pc_df_m = melt(pc_df, id="x") # longform for plotting
plt3 = ggplot(pc_df_m, aes(x=x, y=value, color=variable)) +
    geom_line() +
    labs(x="X", y="Probability", title="PDF and CDF")
plt3

# simulation results
tru = 2^(4/3)*gamma(5/3)
df_plt4 = data.frame(x = ns, y = i3s)
plt4 = ggplot(data=df_plt4, aes(x=x, y=y)) +
    geom_line() +
    geom_hline(yintercept = tru) + # true evaluation
    labs(
         x="Samples",
         y="MC Estimation",
         title=TeX(paste(
            "MC Estimation of",
            "$\\int_0^{\\infty} \\frac{3}{4} x^{4} e^{-x^3/4}$"
            ))
         )
plt4

### END: Code
```

# 2
Here we use importance sampling to approximate
$$I = \frac{1}{\sqrt{2\pi}} \int_{1}^{2} e^{-x^2/2} dx$$
using $f(x)=e^{-x^2/2}/\sqrt{2\pi}$ and $p(x)=\mathbf{1}_{x\in[1,2]}$.
Importance sampling is the approximation of
$$
\mu = E_p(f(x)) \approx \hat{\mu}_g =
\frac{1}{N}\sum_{i=1}^{N} \frac{f(x_i)p(x_i)}{g(x_i)}
$$
where $X_i \sim g(X)$ iid. We rationalize this approximation using the law of
large numbers and the fact that
$$
\mu = E_p(f(x)) = \int_{D}f(x)dp(x) = \int_{D}\frac{f(x)g(x)}{g(x)}dp(x) =
\int_{D}\frac{f(x)p(x)}{g(x)}dg(x) = E_g(\frac{f(x)p(x)}{g(x)})
$$
where $\frac{p(x)}{g(x)}$ is the <i>likelihood ratio</i>,
$g(x)$ is the <i>importance distribution</i> given to be
$g(x) = N(1.5,\nu)$ for $\nu\in\{0.1,1,10\}$, and
$p(x)$ is the <i>nominal distribution</i>.
In this problem, we use $p(x)\sim\mathbf{1}_{[1,2]}$ and
$f(x)=e^{-x^2/2}/\sqrt{2\pi}$.

```{r}
### BEGIN: Code

## Subroutines
g = function(n, u, sd){
    # pdf of importance distribution
    return(dnorm(n, mean=u, sd=sd))
}
p = function(x){
    # pdf of nominal distribution
    if(x<1){return(0)}
    else if(x>2){return(0)}
    else{return(1)}
}
p = Vectorize(p)
f = function(x){
    # pdf of numerator of likelihood ration
    r = exp(-x^2/2)/sqrt(2*pi)
    return(r)
}

## Parameterize
m = 5000 # number of samples
k = 10000 # resolution of sample domain
u = 1.5 # mean of importance sampling
vs = c(0.1, 1, 10) # standard deviations

## Simulate
samples = list()
summands = list()
i = 1
for(v in vs){
    d = c(-1, 1)*(4*v) + u         # sample domain
    domain = seq(d[1], d[2], length.out=k)
    ps = g(domain, u=u, sd=v)
    xs = sample(domain, m, prob=ps, replace=T) # sample from g(x)
    samples[[i]] = xs
    summands[[i]] = (f(xs)*p(xs))/(g(xs, u=u, sd=v))
    i = i + 1
}

### END: Code
```

$$\hat{I}_{\nu=`r vs[1]`}=`r mean(summands[[1]])`$$
$$\hat{I}_{\nu=`r vs[2]`}=`r mean(summands[[2]])`$$
$$\hat{I}_{\nu=`r vs[3]`}=`r mean(summands[[3]])`$$
The true value of the integral is
$$\int_1^2\frac{e^{-x^2/2}}{\sqrt{2\pi}}dx \approx 0.135905$$

Following we observe the histogram of the summands:
```{r}
### BEGIN: Code

## Imports
library(ggplot2, reshape)

## Plot summands

# collect data
df_plt = data.frame(
                    sum1 = summands[[1]],
                    sum2 = summands[[2]],
                    sum3 = summands[[3]]
                )
names(df_plt) = c(
                  paste("v =", vs[1]),
                  paste("v =", vs[2]),
                  paste("v =", vs[3])
                )
df_plt_m = melt(df_plt) # format data

# plot data
plt1 = ggplot(
              data=df_plt_m,
              aes(x=value, color=variable)
            ) +
    geom_histogram(binwidth=0.5, position="dodge") +
    labs(
         x = TeX("$\\frac{f(x_i)p(x_i)}{g_v(x_i)}$"),
         y = "Count",
         title = TeX("Summands of $\\hat{I}_v$")
        )
plt1

# plot only observations larger than 2
plt2 = ggplot(
              data=df_plt_m[df_plt_m$value > 2,],
              aes(x=value, color=variable)
            ) +
    geom_histogram(binwidth=0.5, position="dodge") +
    labs(
         x = TeX("$\\frac{f(x_i)p(x_i)}{g_v(x_i)}$"),
         y = "Count",
         title = TeX("Summands of $\\hat{I}_v when x_i>2$")
        )
plt2
```

We observe that when the importance distribution is too wide or too narrow,
there are many abberant values in the summand for the approximation
$\hat{I} = \frac{1}{N}\sum_{i=1}^N \frac{f(x_i)p(x_i)}{g(x_i)}$.

# 3
$$I=\int_0^1\frac{1}{1+x}dx$$

## 3.a
$h(x)=\frac{1}{1+x}$ and $U_i\sim U[0,1]$ for $i\in\{1,\dots,n\}$.

```{r}
### BEGIN: Code

## Parameterize
N = 1500
us = runif(N, 0, 1)
itru = log(2)
xs = seq(0,1,length.out=N)

## Subroutines
h = function(x){
    if(x<0){return(0)}
    else if(x>1){return(0)}
    else{
        return(1/(1+x))
    }
}
h = Vectorize(h)

## Simulate
i1 = mean(h(us))

## Plot h and int(h)
df_3.a = data.frame(x=xs, y=h(xs))
df_3.a.2 = data.frame(x=xs, y=cumsum((1/N)*h(xs)))
plt_3.a = ggplot() +
    geom_line(data=df_3.a, aes(x=x, y=y), color="coral") +
    geom_line(data=df_3.a.2, aes(x=x, y=y), color="steelblue") +
    # geom_hline(yintercept=i1, color="green") +
    # geom_hline(yintercept=itru, color="blue") +
    labs(title=TeX("$\\frac{1}{1+x}$"))

### END: Code
```

The pink line above is the function $h(x)=\frac{1}{1+x}$
and the blue line is $H(x)=\int_0^x h(t)dt$.
The MCMC estimate of the integral is $\hat{I}_{MC}=`r i1`$
compared with the true integral $I\approx `r itru`$.

## 3.b
```{r}
### BEGIN: Code

cc = function(x){
    if(x<0){return(0)}
    else if(x>1){return(0)}
    else{
        return(1+x)
    }
}
cc = Vectorize(cc)

b = -cov(h(xs), cc(xs)) / var(cc(xs))
i2 = mean(h(us)) + b*(mean(cc(us)) - 1.5)

### END: Code
```

Using the control variate $c(x)=1+x$, we calculate
$\hat{I}_{CV} = \frac{1}{N}\sum_{i=1}^N h(U_i) +
b(\frac{1}{N}\sum_{i=1}^N c(U_i) - E(c(U)))$
where the variance of $\hat{I}_{CV}$ is minimized at
$b=\frac{-cov(h(X), c(X))}{var(c(X))} \approx `r b`$.
Upon calculation, $\hat{I}_{CV}$ is realized as $`r i2`$.

## 3.c
The variance of $\hat{I}_{CV}$ is
$$
Var(\hat{I}_{CV}) =
\frac{1}{N^2}Var(h(X)) - (\frac{b}{N})^2Var(c(X)) =
Var(\hat{I}_{MC}) - \frac{1}{N^2}\frac{Cov(h(X), c(X))^2}{Var(c(X))}
$$

```{r}
### BEGIN: Code

## Parameterize
M = 300 # number of samples for variance estimation

## Simulate
i1s = rep(NA, M)
i2s = rep(NA, M)

for(i in 1:M){
    us = runif(N)
    i1 = mean(h(us))
    i2 = mean(h(us)) + b*(mean(cc(us)) - 1.5)
    i1s[i] = i1
    i2s[i] = i2
}

v1 = var(i1s)
v2 = var(i2s)

### END: Code
```

Running the MC and CV estimators $`r M`$ times with $`r N`$ samples,
$Var(\hat{I}_{MC}) \approx `r v1`$ and
$Var(\hat{I}_{CV}) \approx `r v2`$.

## 3.d
A new estimator for $I$, $\hat{I}_{0}=\log 2$ has
$0$ variance and $0$ bias. Alternatively, we could add another control variate
e.g.
$\hat{I}_{CV2} = \hat{I}_{CV}
+ b_2(\frac{1}{N}\sum_{i=1}^N c_2(U_i) - E(c_2(U)))$
where $c_2(x)=1-\frac{x}{2}$.

# 4
## 4.a
Using the model $y_{ij}=\mu + \alpha_i + e_{ij}$ where
$e_{ij}$ are sampled iid from a double exponential distribution, our null
hypothesis $H_0$ is that $\alpha_i=0$ for all $i\in\{1,\dots,3\}$. The
alternative hypothesis $H_1$ is that, for at least one
$i^\star\in\{1,\dots,3\}$, $\alpha_{i^\star}\ne 0$.

To test the aforementioned null hypothesis $H_0$ using Monte Carlo methods, we
would execute the following algorithm:
```
Algorithm: Monte Carlo test
    input
        Y11...Y1N = Y.1,
        Y21...Y2N = Y.2,
        Y31...Y3N = Y.3,
            Observations for each of three treatments
        M,
            Number of samples for simulated emperical distribution
        K,
            Number of samples for each simulated statistic
        alpha
            Testing significance level
    output
        {0,1}
            0 if fail to reject H_0
            1 if reject H_0

m.hat = mean({Y1...YN})
b.hat = 1/2*sqrt(var(Y1...YN))
d.h0 = doubleExponential.pdf(mean=m.hat, sd=b.hat)

e.cdf = []
for i from 1 to M do:
    x.samp = sample(from=d.h0, n=K)
    x.stat = mean(x.samp)
    e.cdf.append(x.stat)

for i from 1 to 3 do:
    y.i.stat = mean(Y.i)
    if y.i.stat not in quantiles(e.cdf, alpha/2, 1-alpha/2) then:
        return 1
return 0
```

More consisely, we generate a null emperical distribution using parameters
estimated from the observed data and predicated on the $H_0$ model and reject
the null hypothesis if any of the observed means for any of the treatments fall
outside a reasonable simulated emperical quantile.

## 4.b
Alternatively, we could use Fischer's 2-sample permutation method applied to
each pair of treatments. Note that this scales poorly ($O(n!)$) with the number
of treatments.

# 5
## 5.a
We seek to sample $n=100$ times from $Poisson(\lambda r_i)$ with
$r_i\sim Bernoulli(p)$ using $\lambda=2$ and $p=0.3$.

```{r}
### BEGIN: Code

## Parameterize
n = 100
ltrue = 2
ptrue = 0.3

## Sample from prior
r = runif(n) < ptrue # r Bernoulli(p)

## Sample from posterior
xs = r * rpois(n, ltrue)

## Plot
df = data.frame(x=xs)
plt = ggplot(data=df, aes(x=x)) +
    geom_histogram(bins = max(xs)+1) +
    labs(
         x="Value",
         y="Frequency",
         title=TeX(
            "$x_i | \\textbf{r}, \\lambda, p$ when $p=0.3$ and $\\lambda=2$"
            )
        )
plt
```

## 5.b
### 5.b.i

$$
f(\lambda\mid p, r, x) =
\frac{b^a\lambda^{a-1}e^{-b\lambda}}{\Gamma(a)} p^{\sum(r_i)}
(1-p)^{n-\sum(r_i)} \prod\frac{r_i^{x_i}}{x_i!}
e^{-\lambda\sum r_i} \lambda^{\sum x_i} =
$$
$$
\lambda^{a+\sum x_i -1} e^{-(b+\sum r_i)\lambda} \frac{1}{N_{p,r,x}} =
\mathbf{\Gamma}_{a+\sum x_i, b+\sum r_i}(\lambda)
$$

### 5.b.ii

$$
f(\lambda\mid p, r, x) =
\frac{b^a\lambda^{a-1}e^{-b\lambda}}{\Gamma(a)} p^{\sum(r_i)}
(1-p)^{n-\sum(r_i)} \prod\frac{r_i^{x_i}}{x_i!}
e^{-\lambda\sum r_i} \lambda^{\sum x_i} =
$$
$$
p^{\sum r_i}(1-p)^{n-\sum r_i} \frac{1}{N_{\lambda, r, x}} =
\mathbf{\beta}_{\sum r_i, n-\sum r_i}(p)
$$
where
$$
\frac{1}{N_{\lambda, r, x}} =
\frac{b^a\lambda^{a-1}e^{-b\lambda}}{\Gamma(a)}
\prod\frac{r_i^{x_i}}{x_i!}
e^{-\lambda\sum r_i} \lambda^{\sum x_i}
$$

### 5.b.iii

$$
f(\lambda\mid p, r, x) =
\frac{b^a\lambda^{a-1}e^{-b\lambda}}{\Gamma(a)} p^{\sum(r_i)}
(1-p)^{n-\sum(r_i)} \prod\frac{r_i^{x_i}}{x_i!}
e^{-\lambda\sum r_i} \lambda^{\sum x_i} =
$$
$$
(\frac{b^a\lambda^{a-1}e^{-b\lambda}}{\Gamma(a)}) e^{-\lambda r_i}
(\lambda r_i)^{\sum x_i} (\prod\frac{1}{x_i!}) p^{r_i} (1-p)^{1-r_i} =
\frac{1}{N_{\lambda,x,p}} (e^{-\lambda}\frac{p}{1-p})^{r_i} r_i^{\sum x_i} =
$$
$$
Bernoulli(\frac{pe^{-\lambda}}{pe^{-\lambda} + (1-p)\mathbf{1}_{x_i=0}})
$$

## 5.c
To sample from the posterior $f(\lambda, \mathbf{r}, p \mid \mathbf{x})$ we
condition on the sample generated in part (5.a). With this in mind, we expect to
see high density around the values $p=0.3$ and $\lambda=2$ as were used to
generate the sample.

Note that the Jeffreys prior $\mathbf{\beta}(1/2, 1/2)$ is used rather than the
sugguested uniform prior. This is based on a more philosophically sound maximum
entropy principle with respect to parameters regarding which we have no prior
knowledge.

```{r}
### BEGIN: Code

## Parameterize
N = 400 # samples from the Gibbs sampler
A = seq(1/2, 2, by=1/2) # values to use for the prior beta(a,b)(x)
B = A

## Recall that (xs) were generated in a previous snippet

## Instantiate main data frame
df.main = data.frame(matrix(NA,
                            length(A)*length(B)*N, # N samples for each (a,b)
                            (5+n) # i, p, lambda, r1, ..., rn
                        ))
names(df.main) = c("n", "p", "l", "a", "b", paste("r", 1:n, sep=""))

z = 0
for(a in A){for(b in B){

## Initialize lambda, r, p randomly
r0 = runif(n) > rbeta(n, a, b)
l0 = ceiling(runif(1)*max(xs))
p0 = rbeta(1, a, b)
r1 = r0
l1 = l0
p1 = p0

Rs = matrix(NA, N, n)
ls = rep(NA, N)
ps = rep(NA, N)
Rs[1,] = r1
ls[1] = l1
ps[1] = p1

## Run the Gibbs sampler

for(i in 2:N){
    # Update according to conditional distributions
    r1 = runif(n) < (p1*exp(-l0))/(p1*exp(-l0) + (1-p1)*(xs==0))
    l1 = rgamma(1, shape = a + sum(xs), rate = b + sum(r1))
    p1 = rbeta(1, 1 + sum(r1), n + 1 - sum(r1))

    # Record those samples
    Rs[i,] = r1
    ls[i] = l1
    ps[i] = p1
}

# Update the main dataframe (n, p, l, r1, ..., rn)
ixs = (1:N)+z*N
df.main[ixs,][1] = 1:n  # iteration number
df.main[ixs,][2] = ps   # estimate of p
df.main[ixs,][3] = ls   # estimate of lambda
df.main[ixs,][4] = a    # prior a
df.main[ixs,][5] = b    # prior b
df.main[ixs,][6:dim(df.main)[2]] = as.integer(Rs)   # hidden \mathfb{r}_j
z = z + 1

}} # end for A for B

## Plot
plt_lambda = ggplot(data=df.main) +
    geom_line(aes(x=n, y=l), color="steelblue") +
    geom_hline(aes(yintercept=ltrue), color="coral") +
    facet_grid(a ~ b) +
    labs(x="Gibbs sample iteration",
         y=TeX("$\\lambda_i$"),
         title=TeX("Gibbs sampler convergence for $\\lambda$")
        )

# plt_p = ggplot(data=df) +
#     geom_line(aes(x=i, y=p), color="steelblue") +
#     geom_hline(aes(yintercept=ptrue), color="coral") +
#     labs(x="Gibbs sample iteration",
#          y=TeX("$\\p_i$"),
#          title=TeX("Gibbs sampler convergence for $p$")
#         )
# 
# plt_xs = ggplot(melt(matrix(xs)), aes(X1, value)) +
#     geom_point() +
#     labs(
#          x=TeX("Index $i$"),
#          y=TeX("$X_i$"),
#          title=TeX("True observations $\\mathbf{X}$")
#         )
# 
# plt_rn = ggplot(melt(matrix(ceiling(Rs[N,]))), aes(X1, value)) +
#     geom_point() +
#     labs(
#          x=TeX("Index $i$"),
#          y=TeX(paste("$r_{", N, ",i}$", sep="")),
#          title=TeX(paste("True observations $\\mathbf{r_{", N,"}}$", sep=""))
#         )
# 
# plt_rs = ggplot() +
#     geom_raster(
#                 data=melt(ceiling(Rs)),
#                 mapping=aes(X1, X2, fill=value)
#             ) +
#     labs(
#          x="Gibbs sample iteration",
#          y=TeX("$\\mathbf{r}_{j}$"),
#          title=TeX("Gibbs sampler convergence for $\\mathbf{r}$")
#         )

### END: Code
```

```{r echo=FALSE}
plt_lambda
```

```{r echo=FALSE}
library(shiny)
selectInput("data", "",
 c("co2", "lh"))
```

It is interesting to note that $`r sum(Rs[N,] == (xs>0))`$ out of $`r n`$
observations in $\mathbf{r}_{`r N`}$ agree with the $X$ observations generated
in the very beginning.

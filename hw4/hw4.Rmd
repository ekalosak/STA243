---
title: "STA243 Hw4"
author:
- Eric Kalosa-Kenyon
date: "`r format(Sys.time(), '%d %B %Y')`"
abstract: This assignment enforces Monte Carlo methods and exhibits MCMCs. We approximate integrals, perform importance sampling, etc.
output:
  html_document:
    toc: yes
---


# 1
## 1.a
$$\int_0^1 x^2 dx$$
We use $f(x)=x^2$ and $p(x)=1$ uniform over the $[0,1]$ interval.

```{r}
### R version 3.3.2 (2016-10-31) "Sincere Pumpkin Patch"
# x86_64-apple-darwin15.6.0 svn rev 71607

### BEGIN: Code

## Preamble
rm(list=ls())
set.seed(10)

## Imports
library(ggplot2)
library(latex2exp)

## Subroutines
f = function(x){
    return(x^2)
}
p = function(x){
    return(1)
}
f = Vectorize(f)
p = Vectorize(p)

## Parameterize
N = 5000 # Upper limit of samples to average for the MC integral estimation
m = 50 # Sample N/m, 2N/m, .., N times to observe convergence rate
K = 10000 # Resolution on the interval over which to sample w.r.t p(x)
v = 1 # int_0^1 dx = 1

## Simulate
pxs = seq(0, 1, length.out=K)
i1s = rep(NA, m)
ns = (1:m)*N/m
for(i in 1:m){ # m is small so this is fast enough
    xs = sample(pxs, ns[i], prob=p(pxs), replace=T) # sample according to p(x)
    i1 = v*mean(f(xs)) # sample mean of f(x)
    i1s[i] = i1
}

## Plot
df_plt1 = data.frame(x = ns, y = i1s)
plt1 = ggplot(data=df_plt1, aes(x=x, y=y)) +
    geom_line() +
    geom_hline(yintercept = 1/3) + # true evaluation
    labs(
         x="Samples",
         y="MC Estimation",
         title=latex2exp("MC Estimation of \\int_0^1 x^2 dx")
         )
plt1

### END: Code
```

The Monte Carlo estimation of $\int_0^1 x^2 dx$ is $\hat{I}=`r i1`$.
The exact evaluation is $I=\frac{x^3}{3}\bigg\rvert_0^1 = \frac{1}{3}$.
We are off by $\mid I-\hat{I}\mid=`r abs(i1-1/3)`$ which is only a factor of
$\frac{\mid I-\hat{I}\mid}{1/3}=`r abs(i1-1/3)/(1/3)`$.

## 1.b

$$\int_{-2}^2 \int_0^1 x^2 \cos(xy) dxdy$$
We use $f(x,y)=x^2\cos(xy)$ and $p(x,y)=\frac{1}{4}$ uniform over the
$[0,1]\times[-2,2]$ domain.

```{r}
### BEGIN: Code

## Subroutines
f = function(x, y){
    return(x^2*cos(x*y))
}
f = Vectorize(f)

## Parameterize
N = 5000 # Upper limit of samples to average for the MC integral estimation
m = 50 # Sample N/m, 2N/m, .., N times to observe convergence rate
Kx = 1000 # Resolution on the interval over which to sample w.r.t p(x)
Ky = 2000
v = 4 # int_0^1 int_-2^2 dxdy

## Simulate
pxs = seq(0, 1, length.out=Kx)
pys = seq(-2, 2, length.out=Ky)
i2s = rep(NA, m)
ns = (1:m)*N/m
for(i in 1:m){ # m is small so this is fast enough
    xs = sample(pxs, ns[i], replace=T) # sample uniformly
    ys = sample(pys, ns[i], replace=T)
    i2 = v*mean(f(xs, ys)) # sample mean of f(x)
    i2s[i] = i2
}

## Plot
tru = 1/2*(sin(2)-2*cos(2))
df_plt2 = data.frame(x = ns, y = i2s)
plt2 = ggplot(data=df_plt2, aes(x=x, y=y)) +
    geom_line() +
    geom_hline(yintercept = tru) + # true evaluation
    labs(
         x="Samples",
         y="MC Estimation",
         title=latex2exp(paste(
            "MC Estimation of",
            "\\int_{-2}^2\\int_0^1 x^2 cos(xy) dxdy"
            ))
         )
plt2

### END: Code
```

## 1.c

$$\int_{0}^{\infty} \frac{3}{4} x^4 e^{-x^3/4} dx$$
Notice that
$\int_0^{\infty} e^{-x^3/4} dx = 2^{2/3}\Gamma(\frac{4}{3}) = k$ so
we use
$$p(x) = \frac{1}{k}e^{-x^3/4}$$
and
$$f(x) = k\frac{3}{4}x^4$$
When substituted into the original integral, we see
$$
\int_{0}^{\infty} \frac{3}{4} x^4 e^{-x^3/4} dx =
\int_{0}^{\infty} f(x) dp(x)
$$

Hence, we sample from $p(x)$ using inverse CDF sampling and evaluate the samples
at $f(x)$ for a large number of samples following the MC integration method.
Before jumping into the code, however, $P(x)=\int_0^{x}p(y)dy$ must first be
calculated:
$$
P(x) = \int_0^{x}p(y)dy =
\frac{1}{k}(k-\frac{x}{3}E_{2/3}(\frac{y^3}{4}))
$$

where
$$
E_n(x)=\int_1^{\infty} \frac{e^{-xt}}{t^n} dt =
x^{n-1}\Gamma(1-n,x)
$$

and
$$
\Gamma(a,x) = \int_x^\infty t^a e^{-t} dt
$$

For convenience, we calculate the inverse CDF and the CDF numerically.

```{r}
### BEGIN: Code

## Imports
library(reshape)

## Parameterize
N = 10000 # Upper limit of samples to average for the MC integral estimation
m = 50 # Sample N/m, 2N/m, .., N times to observe convergence rate
K = 1000 # Resolution on the interval over which to sample w.r.t p(x)
v = 1 # int_0^inf dp(x)
mx = 4 # point after which p(mx) is practically 0

## Subroutines
k = 2^(2/3)*gamma(4/3)
f = function(x){
    return(k*3/4*x^4)
}
p = function(x){
    return(1/k*exp(-x^3/4))
}
f = Vectorize(f)
p = Vectorize(p)

## Precompute the CDF for inverse sampling
d = mx/K
pc_xs = seq(0, mx, by=d)
pdf = p(pc_xs)
cdf = cumsum(pdf)*d

# must calculate inverse cdf carefully so that it is uniformly sampled over 0,1
us = seq(0, 1, length.out=K) # uniform sampling grid
invcdf = rep(NA, K)
for(i in 1:length(us)){
    ix = sum(cdf<us[i]) + 1
    invcdf[i] = cdf[ix]
}

## Simulate
i3s = rep(NA, m)
ns = (1:m)*N/m

for(i in 1:m){ # m is small so this is fast enough
    xs = sample(pc_xs, ns[i], prob=pdf, replace=T) # sample uniformly
    i3 = v*mean(f(xs)) # sample mean of f(x)
    i3s[i] = i3
}

## Plot
# pdf and cdf
pc_df = data.frame(x=pc_xs, CDF=cdf, PDF=pdf)
pc_df_m = melt(pc_df, id="x") # longform for plotting
plt3 = ggplot(pc_df_m, aes(x=x, y=value, color=variable)) +
    geom_line() +
    labs(x="X", y="Probability", title="PDF and CDF")
plt3

# simulation results
tru = 2^(4/3)*gamma(5/3)
df_plt4 = data.frame(x = ns, y = i3s)
plt4 = ggplot(data=df_plt4, aes(x=x, y=y)) +
    geom_line() +
    geom_hline(yintercept = tru) + # true evaluation
    labs(
         x="Samples",
         y="MC Estimation",
         title=latex2exp(paste(
            "MC Estimation of",
            "\\int_0^{\\infty} \\frac{3}{4} x^{4} e^{-x^3/4}"
            ))
         )
plt4

### END: Code
```

# 2
Here we use importance sampling to approximate
$$I = \frac{1}{\sqrt{2\pi}} \int_{1}^{2} e^{-x^2/2} dx$$
using $f(x)=e^{-x^2/2}/\sqrt{2\pi}$ and $p(x)=\mathbf{1}_{x\in[1,2]}$.
Importance sampling is the approximation of
$$
\mu = E_p(f(x)) \approx \hat{\mu}_g =
\frac{1}{N}\sum_{i=1}^{N} \frac{f(x_i)p(x_i)}{g(x_i)}
$$
where $X_i \sim g(X)$ iid. We rationalize this approximation using the law of
large numbers and the fact that
$$
\mu = E_p(f(x)) = \int_{D}f(x)dp(x) = \int_{D}\frac{f(x)g(x)}{g(x)}dp(x) =
\int_{D}\frac{f(x)p(x)}{g(x)}dg(x) = E_g(\frac{f(x)p(x)}{g(x)})
$$
where $\frac{p(x)}{g(x)}$ is the <i>likelihood ratio</i>,
$g(x)$ is the <i>importance distribution</i> given to be
$g(x) = N(1.5,\nu)$ for $\nu\in\{0.1,1,10\}$, and
$p(x)$ is the <i>nominal distribution</i>.
In this problem, we use $p(x)\sim\mathbf{1}_{[1,2]}$ and
$f(x)=e^{-x^2/2}/\sqrt{2\pi}$.

```{r}
### BEGIN: Code

## Subroutines
g = function(n, u, sd){
    # pdf of importance distribution
    return(dnorm(n, mean=u, sd=sd))
}
p = function(x){
    # pdf of nominal distribution
    if(x<1){return(0)}
    else if(x>2){return(0)}
    else{return(1)}
}
p = Vectorize(p)
f = function(x){
    # pdf of numerator of likelihood ration
    r = exp(-x^2/2)/sqrt(2*pi)
    return(r)
}

## Parameterize
m = 5000 # number of samples
k = 10000 # resolution of sample domain
u = 1.5 # mean of importance sampling
vs = c(0.1, 1, 10) # standard deviations

## Simulate
samples = list()
summands = list()
i = 1
for(v in vs){
    d = c(-1, 1)*(4*v) + u         # sample domain
    domain = seq(d[1], d[2], length.out=k)
    ps = g(domain, u=u, sd=v)
    xs = sample(domain, m, prob=ps, replace=T) # sample from g(x)
    samples[[i]] = xs
    summands[[i]] = (f(xs)*p(xs))/(g(xs, u=u, sd=v))
    i = i + 1
}

### END: Code
```

$$\hat{I}_{\nu=`r vs[1]`}=`r mean(summands[[1]])`$$
$$\hat{I}_{\nu=`r vs[2]`}=`r mean(summands[[2]])`$$
$$\hat{I}_{\nu=`r vs[3]`}=`r mean(summands[[3]])`$$
The true value of the integral is
$$\int_1^2\frac{e^{-x^2/2}}{\sqrt{2\pi}}dx \approx 0.135905$$

Following we observe the histogram of the summands:
```{r}
### BEGIN: Code

## Imports
library(ggplot2, reshape)

## Plot summands

# collect data
df_plt = data.frame(
                    sum1 = summands[[1]],
                    sum2 = summands[[2]],
                    sum3 = summands[[3]]
                )
names(df_plt) = c(
                  paste("v =", vs[1]),
                  paste("v =", vs[2]),
                  paste("v =", vs[3])
                )
df_plt_m = melt(df_plt) # format data

# plot data
plt1 = ggplot(
              data=df_plt_m,
              aes(x=value, color=variable)
            ) +
    geom_histogram(binwidth=0.5, position="dodge") +
    labs(
         x = latex2exp("\\frac{f(x_i)p(x_i)}{g_v(x_i)}"),
         y = "Count",
         title = latex2exp("Summands of \\hat{I}_v")
        )
plt1

# plot only observations larger than 2
plt2 = ggplot(
              data=df_plt_m[df_plt_m$value > 2,],
              aes(x=value, color=variable)
            ) +
    geom_histogram(binwidth=0.5, position="dodge") +
    labs(
         x = latex2exp("\\frac{f(x_i)p(x_i)}{g_v(x_i)}"),
         y = "Count",
         title = latex2exp("Summands of \\hat{I}_v when x_i>2")
        )
plt2
```

We observe that when the importance distribution is too wide or too narrow,
there are many abberant values in the summand for the approximation
$\hat{I} = \frac{1}{N}\sum_{i=1}^N \frac{f(x_i)p(x_i)}{g(x_i)}$.
